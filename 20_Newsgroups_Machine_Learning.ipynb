{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "threatened-details",
   "metadata": {},
   "source": [
    "# Applying Machine Learning To The 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-firmware",
   "metadata": {},
   "source": [
    "The 20 Newsgroups dataset is a corpus of text data downloaded from Usenet groups about 20 years ago and has become a standard in Natural Language Processing projects. The goal of this project is to train a Machine Learning model to classify and predict which entries in the dataset belong to which Newsgroups. Three types of models were tested, a simple Multinomial Naive Bayes classifier, an Artificial Neural Network and a Convolusional Neural Network. As the dataset is considered small by todays standards, the Easy Data Augmentation method was also tested to see if it improves the accuracy of each of the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-attraction",
   "metadata": {},
   "source": [
    "### Import the libraries needed for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "synthetic-manchester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:27:36.384427Z",
     "start_time": "2021-04-05T16:27:36.365714Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "from random import shuffle\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Comment these in if you are running for the first time\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('tagsets')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.datasets import _twenty_newsgroups\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from geneticalgorithm import geneticalgorithm as ga\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from pypet import Environment, cartesian_product, pypetconstants\n",
    "from pypet.parameter import ArrayParameter\n",
    "\n",
    "import smart_open\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-taxation",
   "metadata": {},
   "source": [
    "## Import and process the 20 Newsgroups Dataset\n",
    "\n",
    "The data is stored in 20 text files, one for each Newsgroup and then imported into a single pandas dataframe. Details about the importing of the dataset can be found in the other Jupyter Notebook, ```20_Newsgroups_Exploration.ipynb```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "arabic-vault",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:27:37.383134Z",
     "start_time": "2021-04-05T16:27:36.386320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resulting Dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>newsgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18828</td>\n",
       "      <td>18828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>18828</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>\\nFrom: enis@cbnewsg.cb.att.com (enis.surensoy...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    entry         newsgroup\n",
       "count                                               18828             18828\n",
       "unique                                              18828                20\n",
       "top     \\nFrom: enis@cbnewsg.cb.att.com (enis.surensoy...  rec.sport.hockey\n",
       "freq                                                    1               999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing Newsgroups Manually\n",
    "cwd = os.getcwd()\n",
    "files = glob.glob(cwd + \"/data/raw/*.txt\")\n",
    "\n",
    "# Dictionary to contain the data\n",
    "raw_data_dict = {}\n",
    "\n",
    "# For each newsgroup file\n",
    "for file in files:\n",
    "    # Save the catagory to be used as the dict key\n",
    "    category = file.split(\"/\")[-1].replace(\".txt\", \"\")\n",
    "\n",
    "    # Open the file and save the contents to the dict\n",
    "    with open(file, \"rb\") as datafile:\n",
    "        raw_data_dict[category] = datafile.read().decode(\"iso-8859-1\")\n",
    "\n",
    "# Create a pandas dataframe to contain the data with two columns\n",
    "data_frame = pd.DataFrame(columns=[\"entry\", \"newsgroup\"])\n",
    "\n",
    "# For each Newsgroup\n",
    "for key in raw_data_dict.keys():\n",
    "    # Define the Newsgroup line\n",
    "    divider = \"Newsgroup: \" + key\n",
    "    # print(divider)\n",
    "\n",
    "    # Split the data using the line as the divider\n",
    "    # It is assumed that each unique entry has the Newsgroup in its header\n",
    "    raw_data_dict[key] = raw_data_dict[key].split(divider)\n",
    "\n",
    "    temp_list = []\n",
    "    doc_id_list = []\n",
    "    missing_doc_id = 0\n",
    "\n",
    "    # Filter out any duplicates based on document_id\n",
    "    for i in range(len(raw_data_dict[key])):\n",
    "        # Split the entry into a list\n",
    "        doc_list = raw_data_dict[key][i].split(\"\\n\")\n",
    "\n",
    "        doc_id = False\n",
    "        doc_index = []\n",
    "\n",
    "        # Isolate and save the list element with the document_id\n",
    "        for j, elem in enumerate(doc_list):\n",
    "            # Sometimes document_id can use either an upper or lower case d\n",
    "            if \"ocument_id: \" in elem:\n",
    "                doc_index.append(j)\n",
    "                doc_id = elem.split(\"ocument_id: \")[1]\n",
    "\n",
    "        # If the entry has a document_id\n",
    "        if doc_id:\n",
    "            # And if the document_id is not a duplicate\n",
    "            if doc_id not in doc_id_list:\n",
    "                # Strip the document_id and save the entry to a list\n",
    "                temp_list.append(raw_data_dict[key][i].split(doc_list[doc_index[0]])[1])\n",
    "                # Add the document_id to the list of encountered document_ids\n",
    "                doc_id_list.append(doc_id)\n",
    "\n",
    "        # Otherwise log that the document_id was missing\n",
    "        else:\n",
    "            if len(doc_list) > 1:\n",
    "                missing_doc_id += 1\n",
    "\n",
    "    # Append the entry list to the dataframe with its corresponding Newsgroup\n",
    "    temp_frame = pd.DataFrame(\n",
    "        list(zip(temp_list, [key] * len(temp_list))), columns=[\"entry\", \"newsgroup\"]\n",
    "    )\n",
    "    data_frame = data_frame.append(temp_frame)\n",
    "\n",
    "print(\"\\nResulting Dataframe:\")\n",
    "display(data_frame.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-partner",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "\n",
    "The first thing that needs to be done to the data to prepare it for modeling is to clean it. The most basic processes of text cleaning involves:\n",
    "\n",
    "- Removing any punctuation\n",
    "- Setting all words to lower case\n",
    "- Removing stop words such as ```the```, ```to```, ```and```, etc. \n",
    "   \n",
    "Cleaning reduces the amount of data and its complexity so the model has an easier time processing it. Additionally, words in the dataset can be stemmed or lemmatized to help further reduce the complexity of the dataset. Stemming is the process of reducing a word to it's stem so for example, ```car```, ```cars```, ```car's```, and ```cars'``` will all be reduced to ```car```. The issue with stemming algorithms is that they can be quite crude where stemmed words may not actually be valid words (eg ```something``` would reduce to ```someth```). Lemmatization on the other hand is similar to stemming but in most cases will return a valid word as a result and will try to reduce a word based on the word type (noun, verb, etc.). After some testing, the function ```strip_stopwords_lem``` will be used as the main text cleaning function as it makes for a clean dataset that will work well with later text processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "gorgeous-cursor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:27:37.389546Z",
     "start_time": "2021-04-05T16:27:37.384303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to remove stop words and clean a given string\n",
    "def strip_stopwords(text):\n",
    "    # Remove punctuation and numbers, set all words to lower case and create a list of the words\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    # Create a list of stop words\n",
    "    all_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "    # Remove any stop words\n",
    "    text = [word for word in text if not word in set(all_stopwords)]\n",
    "\n",
    "    # Recreate the string and return\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "# Function to remove stop words, stem words, and clean a given string\n",
    "def strip_stopwords_stem(text):\n",
    "    # Remove punctuation and numbers, set all words to lower case and create a list of the words\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    # Initilize the porterstemmer and create a list of stop words\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "    # Stem all the words in the list and remove any stop words\n",
    "    text = [ps.stem(word) for word in text if not word in set(all_stopwords)]\n",
    "\n",
    "    # Recreate the string and return\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "# Function to remove stop words, lemmatize words, and clean a given string\n",
    "def strip_stopwords_lem(text):\n",
    "    # Remove punctuation and numbers, set all words to lower case and create a list of the words\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    # Initilize the lemmatizer and create a list of stop words\n",
    "    lem = WordNetLemmatizer()\n",
    "    all_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "    # Lemmatize all the words in the list and remove any stop words\n",
    "    text = [lem.lemmatize(word) for word in text if not word in set(all_stopwords)]\n",
    "\n",
    "    # Recreate the string and return\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eligible-ecuador",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:28:08.924825Z",
     "start_time": "2021-04-05T16:27:37.390928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the stop words, punctuation and set all words to lower case\n",
    "data_frame[\"entry-s\"] = data_frame[\"entry\"].apply(strip_stopwords_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-threat",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T16:32:17.599025Z",
     "start_time": "2021-03-25T16:32:17.544666Z"
    }
   },
   "source": [
    "## Set up a Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-tuner",
   "metadata": {},
   "source": [
    "A simple Multinomial Naive Bayes (MNB) model is used as a baseline to compare any changes made to the dataset while preparing it for more complex machine learning techniques. A Naive Bayes model is a probabilistic model that predicts when given an input the probability that it is associated with a given output. The model takes the naive assumption that all the features are independent for a given class which in most cases is not true. However, even though the assumption does not always hold, a Naive Bayes model has been shown to give good results for even complex classification problems. The model was picked over other simple models because it is quick to compute and there are few hyperparameters to choose so no tuning is needed. \n",
    "\n",
    "Before the data can be processed by the model it must first be converted to a list of numbers or vectorized. At first, this is done using the simple CountVectorizer which turns each entry into a list as long as the number of unique words in the dataset containing the frequency of the words in each entry. So for example:```The dog chased the cat``` becomes:```[2, 1, 1, 1]``` which corresponds to: ```[the, dog, chased, cat]```. \n",
    "\n",
    "The model is first trained on the raw data before text cleaning, as a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "novel-analysis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:28:08.930067Z",
     "start_time": "2021-04-05T16:28:08.926842Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a table to keep track of how well models perform\n",
    "score_table = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "alien-turkish",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:28:08.935272Z",
     "start_time": "2021-04-05T16:28:08.931159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to show the top features used by a classifier to make its predictions\n",
    "def show_top10(pipeline, categories):\n",
    "\n",
    "    vectorizer = pipeline.named_steps[\"vectorizer\"]\n",
    "    classifier = pipeline.named_steps[\"classifier\"]\n",
    "\n",
    "    # Save the feature names from the vectorizer as an array\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "\n",
    "    # Define a pandas dataframe to retrun\n",
    "    data_table = pd.DataFrame()\n",
    "\n",
    "    # For each category used in training\n",
    "    for i, category in enumerate(categories):\n",
    "        # Extract the top 10 features from the classifier\n",
    "        top10 = np.argsort(classifier.feature_log_prob_[i])[-10:]\n",
    "\n",
    "        # Append the features to a row of a pandas dataframe\n",
    "        data_row = pd.Series(feature_names[top10], name=category)\n",
    "        data_table = data_table.append(data_row)\n",
    "\n",
    "    # Reset the index to start from 1 and return the dataframe\n",
    "    data_table = data_table.T\n",
    "    data_table.index = data_table.index + 1\n",
    "    return data_table.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-dover",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes using CountVectorizer on the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "sophisticated-winning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:28:18.725559Z",
     "start_time": "2021-04-05T16:28:08.936482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Navie Bayes with CountVectorizer:\n",
      "Training Accuracy: 0.9234\n",
      "Testing Accuracy:  0.8505\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB using CountVectorizer on the raw data\n",
    "MultiNB_Count = Pipeline(\n",
    "    [(\"vectorizer\", CountVectorizer()), (\"classifier\", MultinomialNB())], verbose=False\n",
    ")\n",
    "\n",
    "entries = data_frame[\"entry\"].values\n",
    "y = data_frame[\"newsgroup\"].values\n",
    "\n",
    "# Split the data into test and training groups\n",
    "entries_train, entries_test, y_train, y_test = train_test_split(\n",
    "    entries, y, test_size=0.2, random_state=1234\n",
    ")\n",
    "\n",
    "print(\"Multinomial Navie Bayes with CountVectorizer:\")\n",
    "MultiNB_Count.fit(entries_train, y_train)\n",
    "\n",
    "# Print the score\n",
    "train_score = MultiNB_Count.score(entries_train, y_train)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_score))\n",
    "test_score = MultiNB_Count.score(entries_test, y_test)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_score))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_score, name=\"MultiNB_Count-Raw\", index=[\"Accuracy Score\"])\n",
    ")\n",
    "\n",
    "top_10 = show_top10(MultiNB_Count, sorted(set(data_frame[\"newsgroup\"])))\n",
    "# display(top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-tuner",
   "metadata": {},
   "source": [
    "As you can see, the models accuracy is initially quite high at ~85% accuracy on the testing data. This indicates that there might be some overfitting occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-search",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes using TfidfVectorizer on the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "centered-origin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:28:26.290478Z",
     "start_time": "2021-04-05T16:28:18.727357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Navie Bayes with TfidfVectorizer:\n",
      "Training Accuracy: 0.9276\n",
      "Testing Accuracy:  0.8555\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB using TfidfVectorizer on the raw data\n",
    "MultiNB_Tfidf = Pipeline(\n",
    "    [(\"vectorizer\", TfidfVectorizer()), (\"classifier\", MultinomialNB())], verbose=False\n",
    ")\n",
    "\n",
    "entries = data_frame[\"entry\"].values\n",
    "y = data_frame[\"newsgroup\"].values\n",
    "\n",
    "# Split the data into test and training groups\n",
    "entries_train, entries_test, y_train, y_test = train_test_split(\n",
    "    entries, y, test_size=0.2, random_state=1234\n",
    ")\n",
    "\n",
    "print(\"Multinomial Navie Bayes with TfidfVectorizer:\")\n",
    "MultiNB_Tfidf.fit(entries_train, y_train)\n",
    "\n",
    "# Print the score\n",
    "train_score = MultiNB_Tfidf.score(entries_train, y_train)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_score))\n",
    "test_score = MultiNB_Tfidf.score(entries_test, y_test)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_score))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_score, name=\"MultiNB_Tfidf-Raw\", index=[\"Accuracy Score\"])\n",
    ")\n",
    "\n",
    "top_10 = show_top10(MultiNB_Tfidf, sorted(set(data_frame[\"newsgroup\"])))\n",
    "# display(top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-welsh",
   "metadata": {},
   "source": [
    "Instead of using CountVectorizer, TfidfVectorizer was tested on the raw data to see what affect it has on the accuracy. Like CountVectorizer, TfidfVectorizer (Term Frequency Inverse Document Frequency) counts the words in a given entry but normalizes the count based on the number of times a word appears in a dataset. This has the effect of penalizing words that appear often across a dataset and highlighting words that appear in only a few entries. \n",
    "\n",
    "We can see a marginal improvement in the accuracy using TfidfVectorizer meaning that there is still some overfitting of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-determination",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes using TfidfVectorizer on the data with basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "direct-animation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:28:30.854143Z",
     "start_time": "2021-04-05T16:28:26.292176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Navie Bayes with TfidfVectorizer:\n",
      "Training Accuracy: 0.9421\n",
      "Testing Accuracy:  0.8733\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.81      0.87      0.84       141\n",
      "           comp.graphics       0.79      0.85      0.82       187\n",
      " comp.os.ms-windows.misc       0.88      0.79      0.83       199\n",
      "comp.sys.ibm.pc.hardware       0.88      0.78      0.82       233\n",
      "   comp.sys.mac.hardware       0.83      0.91      0.87       181\n",
      "          comp.windows.x       0.85      0.93      0.89       176\n",
      "            misc.forsale       0.74      0.92      0.82       155\n",
      "               rec.autos       0.92      0.91      0.92       202\n",
      "         rec.motorcycles       0.96      0.95      0.95       206\n",
      "      rec.sport.baseball       0.96      0.98      0.97       218\n",
      "        rec.sport.hockey       0.98      0.95      0.96       215\n",
      "               sci.crypt       0.97      0.88      0.92       229\n",
      "         sci.electronics       0.87      0.87      0.87       193\n",
      "                 sci.med       0.92      0.98      0.95       179\n",
      "               sci.space       0.99      0.87      0.93       186\n",
      "  soc.religion.christian       0.97      0.70      0.81       296\n",
      "      talk.politics.guns       0.98      0.73      0.84       231\n",
      "   talk.politics.mideast       0.98      0.90      0.94       190\n",
      "      talk.politics.misc       0.67      0.98      0.80       114\n",
      "      talk.religion.misc       0.27      0.97      0.42        35\n",
      "\n",
      "                accuracy                           0.87      3766\n",
      "               macro avg       0.86      0.89      0.86      3766\n",
      "            weighted avg       0.90      0.87      0.88      3766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB using TfidfVectorizer on the the data without stop words\n",
    "MultiNB_Tfidf = Pipeline(\n",
    "    [(\"vectorizer\", TfidfVectorizer()), (\"classifier\", MultinomialNB())], verbose=False\n",
    ")\n",
    "\n",
    "entries = data_frame[\"entry-s\"].values\n",
    "y = data_frame[\"newsgroup\"].values\n",
    "\n",
    "# Split the data into test and training groups\n",
    "entries_train, entries_test, y_train, y_test = train_test_split(\n",
    "    entries, y, test_size=0.2, random_state=1234\n",
    ")\n",
    "\n",
    "print(\"Multinomial Navie Bayes with TfidfVectorizer:\")\n",
    "MultiNB_Tfidf.fit(entries_train, y_train)\n",
    "\n",
    "# Print the score\n",
    "train_score = MultiNB_Tfidf.score(entries_train, y_train)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_score))\n",
    "test_score = MultiNB_Tfidf.score(entries_test, y_test)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_score))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_score, name=\"MultiNB_Tfidf-s\", index=[\"Accuracy Score\"])\n",
    ")\n",
    "\n",
    "top_10 = show_top10(MultiNB_Tfidf, sorted(set(data_frame[\"newsgroup\"])))\n",
    "# display(top_10)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "y_pred = MultiNB_Tfidf.predict(entries_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-venture",
   "metadata": {},
   "source": [
    "After removing the stop words from the data there was a further improvement in the accuracy score with the model now achieving over 87% accuracy on average with the accuracy identifying some topics higher than 95%! For such a simple model this appears to be too good to be true and so the data needs to be further processed in order to make the predictions more realistic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-cinema",
   "metadata": {},
   "source": [
    "## Further Data Cleaning\n",
    "\n",
    "The first thing that may be causing such a high accuracy result could be emails in the dataset, perhaps the model is finding repeat users in the dataset and identifying those rather than the substance of the text. To eliminate this possibility all emails are identified and removed from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "united-senate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:28:30.857951Z",
     "start_time": "2021-04-05T16:28:30.855519Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_email(text):\n",
    "    return \" \".join([i for i in text.split(\" \") if \"@\" not in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "mathematical-traffic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:28:57.505559Z",
     "start_time": "2021-04-05T16:28:30.858947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with emails: 18816\n",
      "Number of entries with emails: 0\n"
     ]
    }
   ],
   "source": [
    "# Print the number of emails in the dataset\n",
    "data_frame_email = data_frame[data_frame[\"entry\"].str.contains(\"@\", na=False)]\n",
    "print(\"Number of entries with emails: %i\" % len(data_frame_email))\n",
    "\n",
    "# Remove emails\n",
    "data_frame[\"entry-e\"] = data_frame[\"entry\"].apply(strip_email)\n",
    "\n",
    "# Print any remaining emails in the dataset\n",
    "data_frame_email = data_frame[data_frame[\"entry-e\"].str.contains(\"@\", na=False)]\n",
    "print(\"Number of entries with emails: %i\" % len(data_frame_email))\n",
    "\n",
    "# Remove the stop words, punctuation and set all words to lower case\n",
    "data_frame[\"entry-es\"] = data_frame[\"entry-e\"].apply(strip_stopwords_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-clark",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes using TfidfVectorizer on clean data without emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "harmful-hands",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:03.399009Z",
     "start_time": "2021-04-05T16:28:57.507208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Navie Bayes with TfidfVectorizer:\n",
      "Training Accuracy: 0.9289\n",
      "Testing Accuracy:  0.8609\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB using TfidfVectorizer on the data without emails or stop words\n",
    "MultiNB_Tfidf = Pipeline(\n",
    "    [(\"vectorizer\", TfidfVectorizer()), (\"classifier\", MultinomialNB())], verbose=False\n",
    ")\n",
    "\n",
    "entries = data_frame[\"entry-es\"].values\n",
    "y = data_frame[\"newsgroup\"].values\n",
    "\n",
    "# Split the data into test and training groups\n",
    "entries_train, entries_test, y_train, y_test = train_test_split(\n",
    "    entries, y, test_size=0.2, random_state=1234\n",
    ")\n",
    "\n",
    "print(\"Multinomial Navie Bayes with TfidfVectorizer:\")\n",
    "MultiNB_Tfidf.fit(entries_train, y_train)\n",
    "\n",
    "# Print the score\n",
    "train_score = MultiNB_Tfidf.score(entries_train, y_train)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_score))\n",
    "test_score = MultiNB_Tfidf.score(entries_test, y_test)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_score))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_score, name=\"MultiNB_Tfidf-es\", index=[\"Accuracy Score\"])\n",
    ")\n",
    "\n",
    "top_10 = show_top10(MultiNB_Tfidf, sorted(set(data_frame[\"newsgroup\"])))\n",
    "# display(top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-workstation",
   "metadata": {},
   "source": [
    "#### Removing Headers, Footers and Quotes\n",
    "\n",
    "After removing the emails, the accuracy has reduced but only marginally. At this point, after extensive research it was found that the 20 Newsgroups dataset is actually built into scikit-learn and with it are methods to remove the header, footer and quote blocks from the entries. The reasoning for removing these features is that they are too similar between Newsgroups and the classifier has an easier time learning these features rather than learning to identify the substance of the topics being discussed. The functions to remove the header, footer and quote blocks are applied to the data as well as the function to remove any remaining emails. The text is then cleaned using the ```strip_stopwords_lem``` function from before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "smoking-pioneer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:25.330395Z",
     "start_time": "2021-04-05T16:29:03.401285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove headers using scikit-learn function\n",
    "data_frame[\"entry-h\"] = data_frame[\"entry\"].apply(\n",
    "    _twenty_newsgroups.strip_newsgroup_header\n",
    ")\n",
    "\n",
    "# Remove quotes using scikit-learn function\n",
    "data_frame[\"entry-hq\"] = data_frame[\"entry-h\"].apply(\n",
    "    _twenty_newsgroups.strip_newsgroup_quoting\n",
    ")\n",
    "\n",
    "# Remove footers using scikit-learn function\n",
    "data_frame[\"entry-hqf\"] = data_frame[\"entry-hq\"].apply(\n",
    "    _twenty_newsgroups.strip_newsgroup_footer\n",
    ")\n",
    "\n",
    "# Remove remaining emails\n",
    "data_frame[\"entry-hqfe\"] = data_frame[\"entry-hqf\"].apply(strip_email)\n",
    "\n",
    "# Remove the stop words, punctuation and set all words to lower case\n",
    "data_frame[\"entry-hqfes\"] = data_frame[\"entry-hqfe\"].apply(strip_stopwords_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-asthma",
   "metadata": {},
   "source": [
    "#### Example Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "exterior-agent",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:25.336704Z",
     "start_time": "2021-04-05T16:29:25.331715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry before removing the header, footer and quote block\n",
      "\n",
      "From: et@teal.csn.org (Eric H. Taylor)\n",
      "Subject: Re: Gravity waves, was: Predicting gravity wave quantization & Cosmic Noise\n",
      "\n",
      "In article <C4KvJF.4qo@well.sf.ca.us> metares@well.sf.ca.us (Tom Van Flandern) writes:\n",
      ">crb7q@kelvin.seas.Virginia.EDU (Cameron Randale Bass) writes:\n",
      ">> Bruce.Scott@launchpad.unc.edu (Bruce Scott) writes:\n",
      ">>> \"Existence\" is undefined unless it is synonymous with \"observable\" in\n",
      ">>> physics.\n",
      ">> [crb] Dong ....  Dong ....  Dong ....  Do I hear the death-knell of\n",
      ">> string theory?\n",
      ">\n",
      ">     I agree.  You can add \"dark matter\" and quarks and a lot of other\n",
      ">unobservable, purely theoretical constructs in physics to that list,\n",
      ">including the omni-present \"black holes.\"\n",
      ">\n",
      ">     Will Bruce argue that their existence can be inferred from theory\n",
      ">alone?  Then what about my original criticism, when I said \"Curvature\n",
      ">can only exist relative to something non-curved\"?  Bruce replied:\n",
      ">\"'Existence' is undefined unless it is synonymous with 'observable' in\n",
      ">physics.  We cannot observe more than the four dimensions we know about.\"\n",
      ">At the moment I don't see a way to defend that statement and the\n",
      ">existence of these unobservable phenomena simultaneously.  -|Tom|-\n",
      "\n",
      "\"I hold that space cannot be curved, for the simple reason that it can have\n",
      "no properties.\"\n",
      "\"Of properties we can only speak when dealing with matter filling the\n",
      "space. To say that in the presence of large bodies space becomes curved,\n",
      "is equivalent to stating that something can act upon nothing. I,\n",
      "for one, refuse to subscribe to such a view.\" - Nikola Tesla\n",
      "\n",
      "----\n",
      " ET  \"Tesla was 100 years ahead of his time. Perhaps now his time comes.\"\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "entry after removing the header, footer and quote block\n",
      "\"I hold that space cannot be curved, for the simple reason that it can have\n",
      "no properties.\"\n",
      "\"Of properties we can only speak when dealing with matter filling the\n",
      "space. To say that in the presence of large bodies space becomes curved,\n",
      "is equivalent to stating that something can act upon nothing. I,\n",
      "for one, refuse to subscribe to such a view.\" - Nikola Tesla\n",
      "\n",
      "----\n",
      " ET  \"Tesla was 100 years ahead of his time. Perhaps now his time comes.\"\n"
     ]
    }
   ],
   "source": [
    "# Print an entry before removing the header, footer, quote blocks and emails\n",
    "print(\"Entry before removing the header, footer and quote block\")\n",
    "print(data_frame[\"entry\"].iloc[0])\n",
    "\n",
    "# Print it after removing those features\n",
    "print(\"\\n\\nentry after removing the header, footer and quote block\")\n",
    "print(data_frame[\"entry-hqfe\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-nevada",
   "metadata": {},
   "source": [
    "As you can see, removing these features significantly reduces the size of a given entry. While the scikit-learn functions work well in most cases they are not perfect. In the case above, ```strip_newsgroup_footer``` failed to remove the footer, only removing the dashed line from the bottom. Other examples were explored and in most cases the functions worked well and so applying them should make a difference to the overall accuracy score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-chemistry",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes using TfidfVectorizer on clean data without headers, footers, quotes or emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "verified-course",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:29.480485Z",
     "start_time": "2021-04-05T16:29:25.338371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Navie Bayes with TfidfVectorizer:\n",
      "Training Accuracy: 0.8602\n",
      "Testing Accuracy:  0.7286\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.32      0.75      0.45        65\n",
      "           comp.graphics       0.69      0.66      0.67       212\n",
      " comp.os.ms-windows.misc       0.73      0.66      0.69       200\n",
      "comp.sys.ibm.pc.hardware       0.80      0.64      0.71       255\n",
      "   comp.sys.mac.hardware       0.67      0.84      0.75       159\n",
      "          comp.windows.x       0.80      0.83      0.82       184\n",
      "            misc.forsale       0.72      0.80      0.76       176\n",
      "               rec.autos       0.78      0.79      0.78       197\n",
      "         rec.motorcycles       0.73      0.90      0.81       167\n",
      "      rec.sport.baseball       0.88      0.93      0.91       210\n",
      "        rec.sport.hockey       0.88      0.91      0.89       202\n",
      "               sci.crypt       0.84      0.69      0.76       252\n",
      "         sci.electronics       0.68      0.76      0.72       172\n",
      "                 sci.med       0.81      0.88      0.84       175\n",
      "               sci.space       0.86      0.68      0.76       207\n",
      "  soc.religion.christian       0.94      0.46      0.62       437\n",
      "      talk.politics.guns       0.82      0.61      0.70       233\n",
      "   talk.politics.mideast       0.84      0.73      0.78       201\n",
      "      talk.politics.misc       0.36      0.97      0.52        61\n",
      "      talk.religion.misc       0.00      0.00      0.00         1\n",
      "\n",
      "                accuracy                           0.73      3766\n",
      "               macro avg       0.71      0.72      0.70      3766\n",
      "            weighted avg       0.79      0.73      0.74      3766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB using TfidfVectorizer on the the data without stop words\n",
    "MultiNB_Tfidf = Pipeline(\n",
    "    [(\"vectorizer\", TfidfVectorizer()), (\"classifier\", MultinomialNB())], verbose=False\n",
    ")\n",
    "\n",
    "entries = data_frame[\"entry-hqfes\"].values\n",
    "y = data_frame[\"newsgroup\"].values\n",
    "\n",
    "# Split the data into test and training groups\n",
    "entries_train, entries_test, y_train, y_test = train_test_split(\n",
    "    entries, y, test_size=0.2, random_state=1234\n",
    ")\n",
    "\n",
    "print(\"Multinomial Navie Bayes with TfidfVectorizer:\")\n",
    "MultiNB_Tfidf.fit(entries_train, y_train)\n",
    "\n",
    "# Print the score\n",
    "train_score = MultiNB_Tfidf.score(entries_train, y_train)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_score))\n",
    "test_score = MultiNB_Tfidf.score(entries_test, y_test)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_score))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_score, name=\"MultiNB_Tfidf-hqfes\", index=[\"Accuracy Score\"])\n",
    ")\n",
    "\n",
    "top_10 = show_top10(MultiNB_Tfidf, sorted(set(data_frame[\"newsgroup\"])))\n",
    "# display(top_10)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "y_pred = MultiNB_Tfidf.predict(entries_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-cruise",
   "metadata": {},
   "source": [
    "Now the accuracy score has reduced significantly, reducing to an average of ~73%. This appears to be a more realistic score based on the simplicity of the model. Looking at the classification report, some topics are still being identified easily with scores of over 90% whereas the model is struggling with others. The largest drops are in the Newsgroups with similar topics such as the ```alt.atheism``` and the ```talk.religion``` groups where the model failed to identify more than one entry as from the ```talk.religion``` group. This makes sense as the topics of discussion of these groups are very similar and so the model is having a hard time distinguishing between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "going-stack",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:29.495940Z",
     "start_time": "2021-04-05T16:29:29.484233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultiNB_Count-Raw</th>\n",
       "      <td>0.850505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-Raw</th>\n",
       "      <td>0.855550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-s</th>\n",
       "      <td>0.873340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-es</th>\n",
       "      <td>0.860860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-hqfes</th>\n",
       "      <td>0.728625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy Score\n",
       "MultiNB_Count-Raw          0.850505\n",
       "MultiNB_Tfidf-Raw          0.855550\n",
       "MultiNB_Tfidf-s            0.873340\n",
       "MultiNB_Tfidf-es           0.860860\n",
       "MultiNB_Tfidf-hqfes        0.728625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(score_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-experience",
   "metadata": {},
   "source": [
    "Looking at the final scores after each step of preprocessing, it is clear that each change marginally improved the score until the last step where there was a significant but expected reduction. ~73% accuracy is now the baseline to beat when applying more sophisticated and advanced techniques.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-publisher",
   "metadata": {},
   "source": [
    "## Implementing EDA\n",
    "\n",
    "Easy Data Augmentation (EDA) is the process of augmenting the entries of a dataset in order to improve text classification models. Inspired by similar processes used in machine vision projects, the idea of the technique is that by introducing noise to existing entries and adding them to a dataset, the size of the dataset can be increased giving the model more data to work with. The process was introduced by Jason Wei and Kai Zou in their paper ```EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks``` (which can be found here: https://arxiv.org/pdf/1901.11196.pdf). They use four different methods to randomly apply noise to a dataset:\n",
    "- Synonym Replacement (SR): Words are randomly chosen and replaced with their synonym, with the synonym itself being  chosen at random\n",
    "- Random Insertion (RI): A synonym of a word in the entry is inserted in a random position\n",
    "- Random Swap (RS): Two words are randomly selected and their positions swapped\n",
    "- Random Deletion (RD): Words are randomly deleted from an entry\n",
    "\n",
    "New entries are created by randomly applying one of these techniques to the existing entries and adding the new augmented entry to the dataset. The paper details that on a small dataset, EDA can have a similar affect to using a dataset twice the size. The process of EDA is not built into any data science toolkits and so has been implemented below by modifying the code that the authors used in their paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-profit",
   "metadata": {},
   "source": [
    "#### Synonym Replacement (SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "quick-commerce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:29.511425Z",
     "start_time": "2021-04-05T16:29:29.497258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before synonym replacement:\n",
      "hold space cannot curved simple reason property property speak dealing matter filling space say presence large body space becomes curved equivalent stating something act upon nothing one refuse subscribe view nikola tesla et tesla year ahead time perhaps time come\n",
      "\n",
      "Test after synonym replacement:\n",
      "hold space cannot curved simple reasonableness attribute attribute speak dealing matter filling space say presence large body space becomes curved equivalent state something dissemble upon nothing one defy subscribe view nikola tesla et tesla year ahead time perhaps time come\n"
     ]
    }
   ],
   "source": [
    "# Function to randomly replace words in a string with their synonyms\n",
    "def synonym_replacement(words, n):\n",
    "\n",
    "    # Split the words into a list\n",
    "    words = words.split(\" \")\n",
    "\n",
    "    # Create a copy of the word list\n",
    "    new_words = words.copy()\n",
    "\n",
    "    # Create a randomly shuffled list of the unique words in the entry\n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "\n",
    "    num_replaced = 0\n",
    "\n",
    "    # For each word in the random word list\n",
    "    for random_word in random_word_list:\n",
    "        # Find its synonyms\n",
    "        synonyms = get_synonyms(random_word)\n",
    "\n",
    "        # If there are synonyms for the word\n",
    "        if len(synonyms) >= 1:\n",
    "            # Choose a random sysnonym\n",
    "            synonym = random.choice(list(synonyms))\n",
    "\n",
    "            # Replace each instance of the random word with its synonym\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "\n",
    "            # print(\"\\treplaced\", random_word, \"with\", synonym)\n",
    "\n",
    "            # Increment the counter\n",
    "            num_replaced += 1\n",
    "\n",
    "        # If n number of words have been replaced then stop replacing words\n",
    "        if num_replaced >= n:  # only replace up to n words\n",
    "            break\n",
    "\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "# Function to return a list of synonyms for a given word\n",
    "def get_synonyms(word):\n",
    "\n",
    "    # Define the set of synonyms\n",
    "    synonyms = set()\n",
    "\n",
    "    # For each synonym of the word\n",
    "    for syn in wordnet.synsets(word):\n",
    "        # For each lemma of each synonym\n",
    "        for l in syn.lemmas():\n",
    "            # Remove any non letter characters and add the synonym to the set\n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join(\n",
    "                [char for char in synonym if char in \" qwertyuiopasdfghjklzxcvbnm\"]\n",
    "            )\n",
    "            synonyms.add(synonym)\n",
    "\n",
    "    # If the word itself is in the set of synonyms then remove it\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "\n",
    "    # Return the set of synonyms as a list\n",
    "    return list(synonyms)\n",
    "\n",
    "\n",
    "test = data_frame[\"entry-hqfe\"].iloc[0]\n",
    "test = strip_stopwords_lem(test)\n",
    "\n",
    "print(\"Test before synonym replacement:\")\n",
    "print(test)\n",
    "print(\"\\nTest after synonym replacement:\")\n",
    "print(synonym_replacement(test, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-defendant",
   "metadata": {},
   "source": [
    "#### Random Insertion (RI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "swiss-vietnam",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:29.522948Z",
     "start_time": "2021-04-05T16:29:29.513007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before random insertion:\n",
      "hold space cannot curved simple reason property property speak dealing matter filling space say presence large body space becomes curved equivalent stating something act upon nothing one refuse subscribe view nikola tesla et tesla year ahead time perhaps time come\n",
      "\n",
      "Test after random insertion:\n",
      "hold space cannot curved simple reason property playact property speak dealing physical structure matter filling space say presence large body space becomes curved quadriceps femoris equivalent stating something act upon distribute nothing one refuse subscribe view nikola tesla et tesla year ahead quad time perhaps time come\n"
     ]
    }
   ],
   "source": [
    "# Function to randomly insert a synonym for a word in a string into that string n times\n",
    "def random_insertion(words, n):\n",
    "\n",
    "    # Split the words into a list\n",
    "    words = words.split(\" \")\n",
    "\n",
    "    # Create a copy of the word list\n",
    "    new_words = words.copy()\n",
    "\n",
    "    # For the number of words to be inserted\n",
    "    for _ in range(n):\n",
    "        # Add a new word to the entry\n",
    "        add_word(new_words)\n",
    "\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "# Function to randomly insert a synonym for a word in a string into that string\n",
    "def add_word(new_words):\n",
    "\n",
    "    # Define the synonyms list and counter\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "\n",
    "    # While the number of synonyms is less than 1\n",
    "    while len(synonyms) < 1:\n",
    "        # Choose a random word\n",
    "        random_word = new_words[random.randint(0, len(new_words) - 1)]\n",
    "\n",
    "        # Find its synonyms and increment the counter\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "\n",
    "        # If the counter reaches 10 before finding a synonym, return nothing\n",
    "        if counter >= 10:\n",
    "            return\n",
    "\n",
    "    # Pick the first random synonym\n",
    "    random_synonym = synonyms[0]\n",
    "\n",
    "    # Pick a random spot\n",
    "    random_idx = random.randint(0, len(new_words) - 1)\n",
    "\n",
    "    # Insert the synonym into the random spot\n",
    "    new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "    # print('\\tInserting %s at %i' % (random_synonym, random_idx))\n",
    "\n",
    "\n",
    "test = data_frame[\"entry-hqfe\"].iloc[0]\n",
    "test = strip_stopwords_lem(test)\n",
    "\n",
    "print(\"Test before random insertion:\")\n",
    "print(test)\n",
    "print(\"\\nTest after random insertion:\")\n",
    "print(random_insertion(test, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-washer",
   "metadata": {},
   "source": [
    "#### Random Swap (RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "honey-people",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:29.531798Z",
     "start_time": "2021-04-05T16:29:29.524626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before random swap:\n",
      "hold space cannot curved simple reason property property speak dealing matter filling space say presence large body space becomes curved equivalent stating something act upon nothing one refuse subscribe view nikola tesla et tesla year ahead time perhaps time come\n",
      "\n",
      "Test after random swap:\n",
      "hold space cannot curved simple reason something property speak dealing view filling space say presence large body curved becomes space equivalent stating property act upon nothing one refuse nikola matter subscribe tesla et tesla year ahead time come time perhaps\n"
     ]
    }
   ],
   "source": [
    "# Function to randomly swap two words in a string n times\n",
    "def random_swap(words, n):\n",
    "\n",
    "    # Split the words into a list\n",
    "    words = words.split(\" \")\n",
    "\n",
    "    # Create a copy of the word list\n",
    "    new_words = words.copy()\n",
    "\n",
    "    # For the number of words to be swapped\n",
    "    for _ in range(n):\n",
    "        # Swap words\n",
    "        new_words = swap_word(new_words)\n",
    "\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "# Function to randomly swap two words in a string\n",
    "def swap_word(new_words):\n",
    "\n",
    "    # Pick a random spot in the entry and define a counter\n",
    "    random_idx_1 = random.randint(0, len(new_words) - 1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "\n",
    "    # While the random spots are the same\n",
    "    while random_idx_2 == random_idx_1:\n",
    "\n",
    "        # Pick another random spot and increment the counter\n",
    "        random_idx_2 = random.randint(0, len(new_words) - 1)\n",
    "        counter += 1\n",
    "\n",
    "        # If the counter reaches 3 and another random spot has not been picked then return the list of words\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "\n",
    "    # Swap the two words at the two random spots and return\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = (\n",
    "        new_words[random_idx_2],\n",
    "        new_words[random_idx_1],\n",
    "    )\n",
    "    return new_words\n",
    "\n",
    "\n",
    "test = data_frame[\"entry-hqfe\"].iloc[0]\n",
    "test = strip_stopwords_lem(test)\n",
    "\n",
    "print(\"Test before random swap:\")\n",
    "print(test)\n",
    "print(\"\\nTest after random swap:\")\n",
    "print(random_swap(test, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-sterling",
   "metadata": {},
   "source": [
    "#### Random Deletion (RD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "human-watson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:29.547895Z",
     "start_time": "2021-04-05T16:29:29.539811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before random swap:\n",
      "hold space cannot curved simple reason property property speak dealing matter filling space say presence large body space becomes curved equivalent stating something act upon nothing one refuse subscribe view nikola tesla et tesla year ahead time perhaps time come\n",
      "\n",
      "Test after random swap:\n",
      "hold space cannot simple reason property property speak dealing filling space presence body space becomes curved equivalent stating something act upon nothing one refuse subscribe nikola tesla et tesla year ahead time perhaps time come\n"
     ]
    }
   ],
   "source": [
    "# Function to randomly delete words in a string with a given probabililty p\n",
    "def random_deletion(words, p):\n",
    "\n",
    "    # Split the words into a list\n",
    "    words = words.split(\" \")\n",
    "\n",
    "    # If there's only one word, don't delete it and return\n",
    "    if len(words) == 1:\n",
    "        return \" \".join(words)\n",
    "\n",
    "    new_words = []\n",
    "\n",
    "    # For each word in the list of words\n",
    "    for word in words:\n",
    "        # Pick a random number between 0 and 1\n",
    "        r = random.uniform(0, 1)\n",
    "\n",
    "        # If the number is greater than p, add the word to the new list\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    # If you end up deleting all words, just return a random word\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words) - 1)\n",
    "        return words[rand_int]\n",
    "\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "test = data_frame[\"entry-hqfe\"].iloc[0]\n",
    "test = strip_stopwords_lem(test)\n",
    "\n",
    "print(\"Test before random swap:\")\n",
    "print(test)\n",
    "print(\"\\nTest after random swap:\")\n",
    "print(random_deletion(test, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-cemetery",
   "metadata": {},
   "source": [
    "#### EDA Function\n",
    "\n",
    "The EDA function takes in an entry and returns a given number of augmented entries each associated with its Newsgroup. The function also has four inputs, one for each process of EDA, to control the percentage of words to be replaced/swapped/deleted. The values for these inputs are set to the default found in the EDA paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "periodic-unknown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:29.556268Z",
     "start_time": "2021-04-05T16:29:29.548804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to apply the EDA method to a given entry and its newsgroup\n",
    "def eda(\n",
    "    entry, newsgroup, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9\n",
    "):\n",
    "\n",
    "    # Calculate the number of words in the entry\n",
    "    num_words = len(entry.split(\" \"))\n",
    "\n",
    "    augmented_entries = []\n",
    "\n",
    "    # Calculate the number of new entries per technique (min 1)\n",
    "    num_new_per_technique = int(num_aug / 4) + 1\n",
    "\n",
    "    # Synonym Replacement\n",
    "    if alpha_sr > 0:\n",
    "        # Calculate the number of words to replace (min 1)\n",
    "        n_sr = max(1, int(alpha_sr * num_words))\n",
    "\n",
    "        # For the number of augments\n",
    "        for _ in range(num_new_per_technique):\n",
    "\n",
    "            # Replace a number of words in the entry and add the new entry to the list\n",
    "            a_words = synonym_replacement(entry, n_sr)\n",
    "            augmented_entries.append(a_words)\n",
    "\n",
    "    # Random Insertion\n",
    "    if alpha_ri > 0:\n",
    "        # Calculate the number of words to insert (min 1)\n",
    "        n_ri = max(1, int(alpha_ri * num_words))\n",
    "\n",
    "        # For the number of augments\n",
    "        for _ in range(num_new_per_technique):\n",
    "\n",
    "            # Insert a number of words in the entry and add the new entry to the list\n",
    "            a_words = random_insertion(entry, n_ri)\n",
    "            augmented_entries.append(a_words)\n",
    "\n",
    "    # Random Swap\n",
    "    if alpha_rs > 0:\n",
    "        # Calculate the number of words to swap (min 1)\n",
    "        n_rs = max(1, int(alpha_rs * num_words))\n",
    "\n",
    "        # For the number of augments\n",
    "        for _ in range(num_new_per_technique):\n",
    "\n",
    "            # Swap a number of words in the entry and add the new entry to the list\n",
    "            a_words = random_swap(entry, n_rs)\n",
    "            augmented_entries.append(a_words)\n",
    "\n",
    "    # Random Deletion\n",
    "    if p_rd > 0:\n",
    "\n",
    "        # For the number of augments\n",
    "        for _ in range(num_new_per_technique):\n",
    "\n",
    "            # Delete a percentage of words in the entry and add the new entry to the list\n",
    "            a_words = random_deletion(entry, p_rd)\n",
    "            augmented_entries.append(a_words)\n",
    "\n",
    "    shuffle(augmented_entries)\n",
    "\n",
    "    # trim so that we have the desired number of augmented sentences\n",
    "    if num_aug >= 1:\n",
    "        augmented_entries = augmented_entries[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_entries)\n",
    "        augmented_entries = [\n",
    "            s for s in augmented_entries if random.uniform(0, 1) < keep_prob\n",
    "        ]\n",
    "\n",
    "    # append the original sentence\n",
    "    augmented_entries.append(entry)\n",
    "\n",
    "    # Return the list of entries as a pandas dataframe with its newsgroup\n",
    "    return pd.DataFrame(\n",
    "        list(zip(augmented_entries, [newsgroup] * len(augmented_entries))),\n",
    "        columns=[\"entry\", \"newsgroup\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-physiology",
   "metadata": {},
   "source": [
    "#### Apply EDA and split the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "balanced-device",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:29:29.563137Z",
     "start_time": "2021-04-05T16:29:29.557706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to split a dataset into traning and testing groups and then apply EDA to the training data\n",
    "def eda_split(\n",
    "    entries,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=1234,\n",
    "    alpha_sr=0.1,\n",
    "    alpha_ri=0.1,\n",
    "    alpha_rs=0.1,\n",
    "    p_rd=0.1,\n",
    "    num_aug=9,\n",
    "):\n",
    "    eda_list = []\n",
    "\n",
    "    # Split the data into test and training groups\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        entries, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # For each entry in the training list\n",
    "    for i, entry in enumerate(X_train):\n",
    "        # Apply the EDA method to the entry and append\n",
    "        # the resulting augmented entries to a list\n",
    "        eda_list.append(\n",
    "            eda(\n",
    "                entry,\n",
    "                y_train[i],\n",
    "                alpha_sr=alpha_sr,\n",
    "                alpha_ri=alpha_ri,\n",
    "                alpha_rs=alpha_rs,\n",
    "                p_rd=p_rd,\n",
    "                num_aug=num_aug,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Concatinate the dataframes, shuffle the rows and then extract the entries and newsgroups\n",
    "    eda_data_frame = pd.concat(eda_list)\n",
    "    eda_data_frame = eda_data_frame.sample(frac=1)\n",
    "    X_train_eda = eda_data_frame[\"entry\"].values\n",
    "    y_train_eda = eda_data_frame[\"newsgroup\"].values\n",
    "\n",
    "    return X_train_eda, X_test, y_train_eda, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-trading",
   "metadata": {},
   "source": [
    "## Baseline Model using EDA\n",
    "\n",
    "#### Multinomial Naive Bayes using TfidfVectorizer on cleaned data after applying EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "passing-cookbook",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:30:51.813006Z",
     "start_time": "2021-04-05T16:29:29.565384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Navie Bayes with TfidfVectorizer:\n",
      "Training Accuracy: 0.9341\n",
      "Testing Accuracy:  0.7693\n",
      "Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.56      0.70      0.62       122\n",
      "           comp.graphics       0.72      0.70      0.71       207\n",
      " comp.os.ms-windows.misc       0.74      0.70      0.72       190\n",
      "comp.sys.ibm.pc.hardware       0.80      0.67      0.73       248\n",
      "   comp.sys.mac.hardware       0.72      0.82      0.77       174\n",
      "          comp.windows.x       0.81      0.83      0.82       187\n",
      "            misc.forsale       0.71      0.82      0.76       168\n",
      "               rec.autos       0.81      0.81      0.81       199\n",
      "         rec.motorcycles       0.79      0.87      0.83       185\n",
      "      rec.sport.baseball       0.92      0.93      0.93       220\n",
      "        rec.sport.hockey       0.88      0.93      0.90       200\n",
      "               sci.crypt       0.84      0.78      0.81       224\n",
      "         sci.electronics       0.73      0.74      0.73       189\n",
      "                 sci.med       0.82      0.89      0.85       175\n",
      "               sci.space       0.86      0.71      0.78       199\n",
      "  soc.religion.christian       0.93      0.61      0.74       330\n",
      "      talk.politics.guns       0.83      0.65      0.73       221\n",
      "   talk.politics.mideast       0.87      0.78      0.82       194\n",
      "      talk.politics.misc       0.59      0.86      0.70       114\n",
      "      talk.religion.misc       0.12      0.80      0.22        20\n",
      "\n",
      "                accuracy                           0.77      3766\n",
      "               macro avg       0.75      0.78      0.75      3766\n",
      "            weighted avg       0.80      0.77      0.78      3766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB using TfidfVectorizer on the the data without stop words\n",
    "MultiNB_Tfidf = Pipeline(\n",
    "    [(\"vectorizer\", TfidfVectorizer()), (\"classifier\", MultinomialNB())], verbose=False\n",
    ")\n",
    "\n",
    "entries = data_frame[\"entry-hqfes\"].values\n",
    "y = data_frame[\"newsgroup\"].values\n",
    "\n",
    "# Split the data into test and training groups applying EDA to the training group\n",
    "entries_train, entries_test, y_train, y_test = eda_split(entries, y)\n",
    "\n",
    "print(\"Multinomial Navie Bayes with TfidfVectorizer:\")\n",
    "MultiNB_Tfidf.fit(entries_train, y_train)\n",
    "score = MultiNB_Tfidf.score(entries_test, y_test)\n",
    "\n",
    "# Print the score\n",
    "train_score = MultiNB_Tfidf.score(entries_train, y_train)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_score))\n",
    "test_score = MultiNB_Tfidf.score(entries_test, y_test)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_score))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_score, name=\"MultiNB_Tfidf_EDA-hqfes\", index=[\"Accuracy Score\"])\n",
    ")\n",
    "\n",
    "top_10 = show_top10(MultiNB_Tfidf, sorted(set(data_frame[\"newsgroup\"])))\n",
    "# display(top_10)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "y_pred = MultiNB_Tfidf.predict(entries_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-snowboard",
   "metadata": {},
   "source": [
    "#### EDA Parameter Space Exploration - Grid Search\n",
    "\n",
    "Using the pypet toolkit, the parameter space for the EDA method can be explored. This involves creating an EDA dataset for each permutation of the parameters and running the baseline Multinomial Naive Bayes model on the resulting dataset. Taking about two minutes per run and with about 3750 combinations this is quite computationally expensive and so has not been executed yet. Perhaps the EDA function can be optimized, parallelized or if this script was run on a more powerful computer then the parameter space could be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "primary-village",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:30:51.937112Z",
     "start_time": "2021-04-05T16:30:51.814552Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess pypet.storageservice.HDF5StorageService INFO     I will use the hdf5 file `./HDF/eda_hyper.hdf5`.\n",
      "MainProcess pypet.storageservice.HDF5StorageService INFO     You specified ``overwrite_file=True``, so I deleted the file `./HDF/eda_hyper.hdf5`.\n",
      "MainProcess pypet.environment.Environment INFO     Environment initialized.\n"
     ]
    }
   ],
   "source": [
    "def eda_run(traj):\n",
    "\n",
    "    # Multinomial NB using TfidfVectorizer on the the data without stop words\n",
    "    MultiNB_Tfidf = Pipeline(\n",
    "        [(\"vectorizer\", TfidfVectorizer()), (\"classifier\", MultinomialNB())],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Split the data into test and training groups applying EDA to the training group\n",
    "    entries_train, entries_test, y_train, y_test = eda_split(\n",
    "        traj.entries,\n",
    "        traj.y,\n",
    "        alpha_sr=traj.alpha_sr,\n",
    "        alpha_ri=traj.alpha_ri,\n",
    "        alpha_rs=traj.alpha_rs,\n",
    "        p_rd=traj.p_rd,\n",
    "        num_aug=traj.num_aug,\n",
    "    )\n",
    "\n",
    "    MultiNB_Tfidf.fit(entries_train, y_train)\n",
    "    score = MultiNB_Tfidf.score(entries_test, y_test)\n",
    "\n",
    "    traj.f_add_result(\"score\", score, comment=\"Result of NB\")\n",
    "\n",
    "\n",
    "# Create an environment that handles running our simulation\n",
    "env = Environment(\n",
    "    trajectory=\"EDA_Hyper\",\n",
    "    filename=\"./HDF/eda_hyper.hdf5\",\n",
    "    file_title=\"EDA_Hyper\",\n",
    "    overwrite_file=True,\n",
    "    comment=\"Exploring the hyperparameters of the EDA method\",\n",
    ")\n",
    "\n",
    "# Get the trajectory from the environment\n",
    "traj = env.trajectory\n",
    "\n",
    "# Add parameters\n",
    "traj.f_add_parameter(\n",
    "    \"entries\", list(data_frame[\"entry-hqfes\"].values), comment=\"entries\"\n",
    ")\n",
    "traj.f_add_parameter(\"y\", list(data_frame[\"newsgroup\"].values), comment=\"y values\")\n",
    "\n",
    "traj.f_add_parameter(\"alpha_sr\", 0.0, comment=\"Synonym Replacement alpha\")\n",
    "traj.f_add_parameter(\"alpha_ri\", 0.0, comment=\"Random Insertion alpha\")\n",
    "traj.f_add_parameter(\"alpha_rs\", 0.0, comment=\"Random Swap alpha\")\n",
    "traj.f_add_parameter(\"p_rd\", 0.0, comment=\"Random Deletion probability\")\n",
    "traj.f_add_parameter(\"num_aug\", 0, comment=\"Number of Augmented Entries\")\n",
    "\n",
    "param_space = {\n",
    "    \"alpha_sr\": [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \"alpha_ri\": [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \"alpha_rs\": [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \"p_rd\": [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \"num_aug\": [2, 4, 6, 8, 10, 12],\n",
    "}\n",
    "\n",
    "# space_product = cartesian_product(param_space)\n",
    "# print(len(space_product['alpha_sr']))\n",
    "\n",
    "# Explore the parameters with a cartesian product\n",
    "traj.f_explore(cartesian_product(param_space))\n",
    "\n",
    "# Run the simulation with all parameter combinations\n",
    "# env.run(eda_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-cement",
   "metadata": {},
   "source": [
    "#### EDA Parameter Space Exploration - Genetic Algorithm\n",
    "\n",
    "Alternatively the parameter space for the EDA method could be explored using a genetic algorithm. A genetic algorithm (GA) takes a set of random combinations of parameters called a population and compares them to one another finding the best preforming combination in a population. The parameters, also called genes, that make up the best individuals are then passed on to the next generation where the process repeats. However, before the genes are passed on, some random fluctuations called mutations are applied to the genes to add some variability to the next generation. Iterating through this process of selection and mutation the algorithm should converge on an optimum result. This process can be quicker than a standard coarse grid search as was implemented above but is still expected to take several days to run so as of now it has not been tested. The geneticalgorithm library which implements an elitist genetic algorithm is used rather than implementing a GA from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "synthetic-symphony",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:30:51.945249Z",
     "start_time": "2021-04-05T16:30:51.938160Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global parameters which the EDA function uses. They cannot be given to the funtion \n",
    "# directly as it must only accept an array of numbers for the GA library to work\n",
    "entries = data_frame[\"entry-hqfes\"].values\n",
    "y = data_frame[\"newsgroup\"].values\n",
    "\n",
    "# Function to create and train an EDA dataset for given parameters \n",
    "def eda_run(X):\n",
    "    \n",
    "    # Multinomial NB using TfidfVectorizer on the the data without stop words\n",
    "    MultiNB_Tfidf = Pipeline(\n",
    "        [(\"vectorizer\", TfidfVectorizer()), (\"classifier\", MultinomialNB())],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Split the data into test and training groups applying EDA to the training group\n",
    "    entries_train, entries_test, y_train, y_test = eda_split(\n",
    "        entries,\n",
    "        y,\n",
    "        alpha_sr=X[0]/10,\n",
    "        alpha_ri=X[1]/10,\n",
    "        alpha_rs=X[2]/10,\n",
    "        p_rd=X[3]/10,\n",
    "        num_aug=int(X[4]),\n",
    "    )\n",
    "\n",
    "    MultiNB_Tfidf.fit(entries_train, y_train)\n",
    "    score = MultiNB_Tfidf.score(entries_test, y_test)\n",
    "    \n",
    "    # print(X, score)\n",
    "    \n",
    "    return -score\n",
    "\n",
    "\n",
    "# Define the parameter space for each parameter \n",
    "varbound = np.array([[0, 5], [0, 5], [0, 5], [0, 5], [2, 12]])\n",
    "vartype = np.array([[\"int\"], [\"int\"], [\"int\"], [\"int\"], [\"int\"]])\n",
    "\n",
    "# Set the parameters for the algorithm\n",
    "algorithm_param = {\n",
    "    \"max_num_iteration\": 100,\n",
    "    \"population_size\": 20,\n",
    "    \"mutation_probability\": 0.1,\n",
    "    \"elit_ratio\": 0.01,\n",
    "    \"crossover_probability\": 0.5,\n",
    "    \"parents_portion\": 0.3,\n",
    "    \"crossover_type\": \"uniform\",\n",
    "    \"max_iteration_without_improv\": 5,\n",
    "}\n",
    "\n",
    "# Define the algorithm and execute it\n",
    "model = ga(\n",
    "    function=eda_run,\n",
    "    dimension=5,\n",
    "    variable_type_mixed=vartype,\n",
    "    variable_boundaries=varbound,\n",
    "    algorithm_parameters=algorithm_param,\n",
    "    function_timeout=600\n",
    ")\n",
    "\n",
    "# model.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-london",
   "metadata": {},
   "source": [
    "After running a coarse GA test the algorithm began to converge on a result of:\n",
    "\n",
    "- 20% Synonym Replacement \n",
    "- 50% Random Insertion  \n",
    "- 10% random Swap \n",
    "- 40% Random Deletion Chance \n",
    "- 11 Augmented Sentences\n",
    "\n",
    "A more in depth test could be carried out but testing out these values should see an improved result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-bundle",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes using TfidfVectorizer on cleaned data after applying optimized EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "optimum-boulder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:34:07.868773Z",
     "start_time": "2021-04-05T16:30:51.946406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Navie Bayes with TfidfVectorizer:\n",
      "Training Accuracy: 0.9234\n",
      "Testing Accuracy:  0.7685\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB using TfidfVectorizer on the the data without stop words\n",
    "MultiNB_Tfidf = Pipeline(\n",
    "    [(\"vectorizer\", TfidfVectorizer()), (\"classifier\", MultinomialNB())], verbose=False\n",
    ")\n",
    "\n",
    "entries = data_frame[\"entry-hqfes\"].values\n",
    "y = data_frame[\"newsgroup\"].values\n",
    "\n",
    "# Split the data into test and training groups applying EDA to the training group\n",
    "entries_train, entries_test, y_train, y_test = eda_split(entries, y, alpha_sr=0.2, alpha_ri=0.5, alpha_rs=0.1, p_rd=0.4, num_aug=11)\n",
    "\n",
    "print(\"Multinomial Navie Bayes with TfidfVectorizer:\")\n",
    "MultiNB_Tfidf.fit(entries_train, y_train)\n",
    "score = MultiNB_Tfidf.score(entries_test, y_test)\n",
    "\n",
    "# Print the score\n",
    "train_score = MultiNB_Tfidf.score(entries_train, y_train)\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_score))\n",
    "test_score = MultiNB_Tfidf.score(entries_test, y_test)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_score))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_score, name=\"MultiNB_Tfidf_oEDA-hqfes\", index=[\"Accuracy Score\"])\n",
    ")\n",
    "\n",
    "top_10 = show_top10(MultiNB_Tfidf, sorted(set(data_frame[\"newsgroup\"])))\n",
    "# display(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "convinced-roads",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:34:07.875476Z",
     "start_time": "2021-04-05T16:34:07.870125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultiNB_Count-Raw</th>\n",
       "      <td>0.850505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-Raw</th>\n",
       "      <td>0.855550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-s</th>\n",
       "      <td>0.873340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-es</th>\n",
       "      <td>0.860860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-hqfes</th>\n",
       "      <td>0.728625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf_EDA-hqfes</th>\n",
       "      <td>0.769251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf_oEDA-hqfes</th>\n",
       "      <td>0.768455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Accuracy Score\n",
       "MultiNB_Count-Raw               0.850505\n",
       "MultiNB_Tfidf-Raw               0.855550\n",
       "MultiNB_Tfidf-s                 0.873340\n",
       "MultiNB_Tfidf-es                0.860860\n",
       "MultiNB_Tfidf-hqfes             0.728625\n",
       "MultiNB_Tfidf_EDA-hqfes         0.769251\n",
       "MultiNB_Tfidf_oEDA-hqfes        0.768455"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(score_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-guard",
   "metadata": {},
   "source": [
    "Using the EDA method the resulting accuracy score increased from ~73% to ~77%. This is a significant increase indicating that the original dataset was indeed small and that with more entries a more accurate result can be achieved. Only a coarse exploration of the EDA parameter space was explored using the genetic algorithm, in the future a more robust exploration should be conducted to see if any more improvements could be made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-obligation",
   "metadata": {},
   "source": [
    "## Deep Learning Models\n",
    "\n",
    "Now that an adequate score has been achieved using the baseline model with the amount of text preprocessing done, the augmented dataset can now be used to train more sophisticated machine learning models. Two types of Deep Learning models are tested, a simple Artificial Neural Network and a Convolutional Neural Network. A Recurrent Neural Network was also implemented but never tested as it failed in training because it is too computationally expensive to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-shepherd",
   "metadata": {},
   "source": [
    "#### Define the dataset \n",
    "\n",
    "Since training takes a long time, the resulting models are saved after training. The problem with saving the models is that the data they use is randomly generated and so the next time the model is loaded, the dataset would have changed. To fix this issue, the data is also saved along with the models and loaded the next time that a model is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "relative-governor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:36:39.021540Z",
     "start_time": "2021-04-05T16:34:07.877001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Newsgroups sanity check: 20\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the dataframe and extract the entries and Newsgroups\n",
    "data_frame_shuffle = data_frame.sample(frac=1, random_state=1234)\n",
    "entries = data_frame_shuffle[\"entry-hqfes\"].values\n",
    "y = data_frame_shuffle[\"newsgroup\"].values\n",
    "\n",
    "# Define a percentage of the dataset to run on\n",
    "# This is used to speed up processing during development and should be set to 1\n",
    "percentage = 1\n",
    "data_len = int(len(entries) * percentage)\n",
    "entries = entries[:data_len]\n",
    "y = y[:data_len]\n",
    "\n",
    "# Label encode the Newsgroups\n",
    "encoder = LabelEncoder()\n",
    "y_label = encoder.fit_transform(y)\n",
    "\n",
    "# The label encoded Newsgroups are then one hot encoded\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_label = y_label.reshape((len(y_label), 1))\n",
    "y_ohe = encoder.fit_transform(y_label)\n",
    "\n",
    "# Print the number of Newsgroups in the dataset\n",
    "# This is the check that a slice contains all the Newsgroups\n",
    "print(\"Number of Newsgroups sanity check:\", len(set(y)))\n",
    "\n",
    "# Define the number of epochs, the patience of the early stopping algorithims\n",
    "# and the batch size for all models\n",
    "epochs = 100\n",
    "patience = max(2, epochs / 10)\n",
    "batch = 128\n",
    "\n",
    "# A flag used in the below cells to dictate when a network is imported or trained from scratch\n",
    "import_model = False\n",
    "\n",
    "entries_filename = \"data/entries\" + str(percentage * 100) + \".pickle\"\n",
    "eda_filename = \"data/entries_eda\" + str(percentage * 100) + \".pickle\"\n",
    "\n",
    "# If import_model is true and the data has been saved before then import the data\n",
    "if import_model and os.path.exists(entries_filename):\n",
    "    with open(entries_filename, \"rb\") as entryfile:\n",
    "        entries_train, entries_test, y_train, y_test = pickle.load(entryfile)\n",
    "\n",
    "    with open(eda_filename, \"rb\") as edafile:\n",
    "        entries_train_eda, entries_test_eda, y_train_eda, y_test_eda = pickle.load(\n",
    "            edafile\n",
    "        )\n",
    "\n",
    "# Otherwise split the data into training as test sets and pickle save them\n",
    "else:\n",
    "    # Split the data into test and training groups\n",
    "    entries_train, entries_test, y_train, y_test = train_test_split(\n",
    "        entries, y_ohe, test_size=0.2, random_state=1234\n",
    "    )\n",
    "\n",
    "    # Split the data into test and training groups using EDA\n",
    "    entries_train_eda, entries_test_eda, y_train_eda, y_test_eda = eda_split(\n",
    "        entries, y_ohe, alpha_sr=0.2, alpha_ri=0.5, alpha_rs=0.1, p_rd=0.4, num_aug=11,\n",
    "    )\n",
    "\n",
    "    with open(entries_filename, \"wb\") as entryfile:\n",
    "        data = (entries_train, entries_test, y_train, y_test)\n",
    "        pickle.dump(data, entryfile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(eda_filename, \"wb\") as edafile:\n",
    "        eda_data = (entries_train_eda, entries_test_eda, y_train_eda, y_test_eda)\n",
    "        pickle.dump(eda_data, edafile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-completion",
   "metadata": {},
   "source": [
    "### Artificial Neural Network\n",
    "\n",
    "The first model to be tested is a simple Artificial Neural Network (ANN). An ANN is a network of connected nodes called neurons which function together to relate a given input to a predicted output. These neurons are stacked into layers and are connected to one another where the output of one neuron can be the input to multiple neurons. Each neuron itself contains an activation function which dictates if the neuron has been 'activated' based on the sum of its inputs. Through the process of adjusting the ways that these neurons are connected to one another an ANN can be trained to recognize patterns in data such as identify subjects in photos or classify texts, as it will be used in this case. \n",
    "\n",
    "In order to train the network the Newsgroup categories must first be one hot encoded. One hot encoding is the process of converting a list of categories into vectors so the neural network can better understand the relationship between the inputs and the outputs. For this model, TfidfVectorizer is again used to encode the entries. \n",
    "\n",
    "The length of the input to the neural network is of the order ~75000 (the dataset vocab) and the output is length 20, one output for each Newsgroup. Initially a single hidden layer with 1000 neurons was chosen based on a rule of thumb which says:\n",
    "\n",
    "    number of hidden nodes ~ sqrt(input layer nodes * output layer nodes)\n",
    "    \n",
    "but these hyperparameters will be tuned to get the best result. The model is first trained on the cleaned data and then trained again on the EDA data to see what affect it has on its accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bigger-assault",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:36:41.182050Z",
     "start_time": "2021-04-05T16:36:39.022658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the vectorizer to the training data\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(entries_train)\n",
    "\n",
    "# Transform the data using the training data fit\n",
    "X_train = vectorizer.transform(entries_train)\n",
    "X_test = vectorizer.transform(entries_test)\n",
    "\n",
    "# The indices of the sparse matrices are then sorted\n",
    "X_train.sort_indices()\n",
    "X_test.sort_indices()\n",
    "\n",
    "input_dim = X_train.shape[1]  # Number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-thanksgiving",
   "metadata": {},
   "source": [
    "Note: The training and test data are returned as sparse matrices, initially with their indices out of order. The tensorflow backend to keras will not run when the indices are out of order and so they are sorted using ```sort_indices()```. By reordering the indices this has the affect of reordering the entries but since the model does not take into consideration the order of the words in each entry it is thought that this shouldn't have an affect on the overall accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "historic-brand",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:36:41.186806Z",
     "start_time": "2021-04-05T16:36:41.183151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def ANN_model(input_dim, neurons=1000, hidden=1):\n",
    "    model = Sequential()\n",
    "\n",
    "    for i in range(hidden):\n",
    "        model.add(layers.Dense(neurons, input_dim=input_dim, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(20, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    # model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define EarlyStopping function\n",
    "es = EarlyStopping(monitor=\"loss\", mode=\"min\", patience=patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-fighter",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "\n",
    "There are multiple parameters to choose when defining a machine learning model. For now, the best way to optimize a model is to test multiple values for each parameter to see which set gives the best results. In the case of an ANN, the number of layers in the network and the number of neurons in each layer can be tuned. In this case, 1 to 3 layers were tested each containing 100-1500 neurons going up in steps of 100 neurons. The process of training each model takes about 20 minutes on my local PC using CPU, so the hyperparameter tuning was done using Google Colab since a more powerful GPU could be used. The best results from Google Colab are then used to train a model on this system.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "sorted-burst",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "model = KerasClassifier(build_fn=ANN_model, epochs=epochs, batch_size=batch)\n",
    "\n",
    "neurons = [\n",
    "    100,\n",
    "    200,\n",
    "    300,\n",
    "    400,\n",
    "    500,\n",
    "    600,\n",
    "    700,\n",
    "    800,\n",
    "    900,\n",
    "    1000,\n",
    "    1100,\n",
    "    1200,\n",
    "    1300,\n",
    "    1400,\n",
    "    1500,\n",
    "]\n",
    "hidden = [1, 2, 3]\n",
    "param_grid = dict(input_dim=[input_dim], neurons=neurons, hidden=hidden)\n",
    "print(param_grid)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, verbose=1)\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    # fit network\n",
    "    results = grid.fit(X_train, y_train, callbacks=[es])\n",
    "\n",
    "print(\"Best: %f using %s\" % (results.best_score_, results.best_params_))\n",
    "means = results.cv_results_[\"mean_test_score\"]\n",
    "stds = results.cv_results_[\"std_test_score\"]\n",
    "params = results.cv_results_[\"params\"]\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-publication",
   "metadata": {},
   "source": [
    "#### Artificial Neural Network using TfidfVectorizer on cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "durable-gibraltar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:49:56.836225Z",
     "start_time": "2021-04-05T16:36:41.187972Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 250)               15722500  \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                5020      \n",
      "=================================================================\n",
      "Total params: 15,727,520\n",
      "Trainable params: 15,727,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "118/118 [==============================] - 13s 105ms/step - loss: 2.7273 - accuracy: 0.4914\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 13s 113ms/step - loss: 1.0669 - accuracy: 0.8626\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 13s 111ms/step - loss: 0.4426 - accuracy: 0.9407\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 0.2345 - accuracy: 0.9692\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 12s 101ms/step - loss: 0.1313 - accuracy: 0.9860\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 0.0829 - accuracy: 0.9909\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 13s 109ms/step - loss: 0.0620 - accuracy: 0.9922\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 0.0506 - accuracy: 0.9924\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 0.0440 - accuracy: 0.9920\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 0.0376 - accuracy: 0.9930\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 0.0322 - accuracy: 0.9933\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 13s 109ms/step - loss: 0.0285 - accuracy: 0.9943\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 0.0240 - accuracy: 0.9953\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 0.0232 - accuracy: 0.9948\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 0.0250 - accuracy: 0.9933\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 13s 109ms/step - loss: 0.0235 - accuracy: 0.9937\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 13s 105ms/step - loss: 0.0183 - accuracy: 0.9951\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 12s 100ms/step - loss: 0.0221 - accuracy: 0.9945\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 0.0217 - accuracy: 0.9941\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 0.0205 - accuracy: 0.9942\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 13s 108ms/step - loss: 0.0195 - accuracy: 0.9947\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 0.0208 - accuracy: 0.9939\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 0.0206 - accuracy: 0.9941\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 12s 104ms/step - loss: 0.0216 - accuracy: 0.9939\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 12s 105ms/step - loss: 0.0177 - accuracy: 0.9947\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 13s 110ms/step - loss: 0.0216 - accuracy: 0.9937\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 12s 103ms/step - loss: 0.0172 - accuracy: 0.9947\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 12s 102ms/step - loss: 0.0186 - accuracy: 0.9947\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 13s 106ms/step - loss: 0.0167 - accuracy: 0.9950\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 13s 108ms/step - loss: 0.0183 - accuracy: 0.9943\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 11s 94ms/step - loss: 0.0188 - accuracy: 0.9941\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0195 - accuracy: 0.9939\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0163 - accuracy: 0.9953\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 11s 91ms/step - loss: 0.0148 - accuracy: 0.9954\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 11s 91ms/step - loss: 0.0183 - accuracy: 0.9943\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 11s 91ms/step - loss: 0.0186 - accuracy: 0.9943\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0182 - accuracy: 0.9946\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0204 - accuracy: 0.9939\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0164 - accuracy: 0.9951\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 11s 89ms/step - loss: 0.0170 - accuracy: 0.9944\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 11s 93ms/step - loss: 0.0195 - accuracy: 0.9940\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 11s 96ms/step - loss: 0.0182 - accuracy: 0.9944\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 11s 94ms/step - loss: 0.0186 - accuracy: 0.9945\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0152 - accuracy: 0.9952\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 11s 94ms/step - loss: 0.0176 - accuracy: 0.9944\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 11s 94ms/step - loss: 0.0163 - accuracy: 0.9950\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 11s 96ms/step - loss: 0.0166 - accuracy: 0.9943\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 11s 91ms/step - loss: 0.0184 - accuracy: 0.9945\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 11s 93ms/step - loss: 0.0208 - accuracy: 0.9935\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 11s 94ms/step - loss: 0.0172 - accuracy: 0.9942\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0171 - accuracy: 0.9948\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0165 - accuracy: 0.9946\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 12s 99ms/step - loss: 0.0177 - accuracy: 0.9940\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 11s 96ms/step - loss: 0.0160 - accuracy: 0.9951\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 11s 94ms/step - loss: 0.0136 - accuracy: 0.9957\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 11s 93ms/step - loss: 0.0168 - accuracy: 0.9945\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 11s 91ms/step - loss: 0.0157 - accuracy: 0.9950\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 11s 95ms/step - loss: 0.0178 - accuracy: 0.9943\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0195 - accuracy: 0.9935\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 11s 94ms/step - loss: 0.0192 - accuracy: 0.9943\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 11s 90ms/step - loss: 0.0160 - accuracy: 0.9951\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 11s 91ms/step - loss: 0.0147 - accuracy: 0.9951\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 11s 91ms/step - loss: 0.0167 - accuracy: 0.9946\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 11s 95ms/step - loss: 0.0129 - accuracy: 0.9958\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 11s 90ms/step - loss: 0.0172 - accuracy: 0.9942\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 11s 95ms/step - loss: 0.0173 - accuracy: 0.9942\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 11s 93ms/step - loss: 0.0160 - accuracy: 0.9953\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 11s 92ms/step - loss: 0.0187 - accuracy: 0.9940\n",
      "INFO:tensorflow:Assets written to: models/ANN_Tfidf-hqfes_100128100/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess tensorflow INFO     Assets written to: models/ANN_Tfidf-hqfes_100128100/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9944\n",
      "Testing Accuracy:  0.7650\n"
     ]
    }
   ],
   "source": [
    "model = ANN_model(input_dim, neurons=250, hidden=1)\n",
    "model.summary()\n",
    "\n",
    "model_name = \"ANN_Tfidf-hqfes_\" + str(epochs) + str(batch) + str(percentage * 100)\n",
    "\n",
    "if import_model and os.path.exists(\"models/\" + model_name):\n",
    "    model = load_model(\"models/\" + model_name)\n",
    "else:\n",
    "    # fit network\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        verbose=True,\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "        callbacks=[es],\n",
    "    )\n",
    "    model.save(\"models/\" + model_name)\n",
    "\n",
    "# Print the score\n",
    "train_loss, train_accuracy = model.evaluate(\n",
    "    X_train, y_train, batch_size=128, verbose=False\n",
    ")\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_accuracy, name=model_name, index=[\"Accuracy Score\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-compatibility",
   "metadata": {},
   "source": [
    "After extensive testing, ANNs with a single layer and a neuron count of between 100 and 500 all performed similarly so a value of 250 was chosen. This model produces a result of ~76.5% which is about as good as the MNB model using EDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-science",
   "metadata": {},
   "source": [
    "#### Neural Network using TfidfVectorizer on cleaned EDA data \n",
    "\n",
    "Now that the optimum values for the network have been chosen, the model can be trained again on the data after applying EDA to see what affect the method has on the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "arctic-cotton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:50:17.864024Z",
     "start_time": "2021-04-05T16:49:56.841035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the vectorizer to the training data\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(entries_train_eda)\n",
    "\n",
    "# Transform the data using the training data fir\n",
    "X_train = vectorizer.transform(entries_train_eda)\n",
    "X_test = vectorizer.transform(entries_test_eda)\n",
    "\n",
    "# The indices of the sparse matrices are then sorted\n",
    "X_train.sort_indices()\n",
    "X_test.sort_indices()\n",
    "\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "# For some reason the output of eda_split does not work when fitting so must be converted to numpy arrays\n",
    "y_train_eda = np.array([np.array(x) for x in y_train_eda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "presidential-paragraph",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T19:19:58.605568Z",
     "start_time": "2021-04-05T16:50:17.868131Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 250)               18794750  \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                5020      \n",
      "=================================================================\n",
      "Total params: 18,799,770\n",
      "Trainable params: 18,799,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1413/1413 [==============================] - 155s 109ms/step - loss: 1.0686 - accuracy: 0.8185\n",
      "Epoch 2/100\n",
      "1413/1413 [==============================] - 154s 109ms/step - loss: 0.0439 - accuracy: 0.9901\n",
      "Epoch 3/100\n",
      "1413/1413 [==============================] - 173s 122ms/step - loss: 0.0263 - accuracy: 0.9931\n",
      "Epoch 4/100\n",
      "1413/1413 [==============================] - 178s 126ms/step - loss: 0.0217 - accuracy: 0.9938\n",
      "Epoch 5/100\n",
      "1413/1413 [==============================] - 178s 126ms/step - loss: 0.0202 - accuracy: 0.9941\n",
      "Epoch 6/100\n",
      "1413/1413 [==============================] - 173s 122ms/step - loss: 0.0204 - accuracy: 0.9940\n",
      "Epoch 7/100\n",
      "1413/1413 [==============================] - 157s 111ms/step - loss: 0.0186 - accuracy: 0.9942\n",
      "Epoch 8/100\n",
      "1413/1413 [==============================] - 153s 108ms/step - loss: 0.0189 - accuracy: 0.9941\n",
      "Epoch 9/100\n",
      "1413/1413 [==============================] - 153s 108ms/step - loss: 0.0189 - accuracy: 0.9941\n",
      "Epoch 10/100\n",
      "1413/1413 [==============================] - 154s 109ms/step - loss: 0.0179 - accuracy: 0.9945\n",
      "Epoch 11/100\n",
      "1413/1413 [==============================] - 152s 107ms/step - loss: 0.0179 - accuracy: 0.9944\n",
      "Epoch 12/100\n",
      "1413/1413 [==============================] - 154s 109ms/step - loss: 0.0182 - accuracy: 0.9943\n",
      "Epoch 13/100\n",
      "1413/1413 [==============================] - 154s 109ms/step - loss: 0.0177 - accuracy: 0.9944\n",
      "Epoch 14/100\n",
      "1413/1413 [==============================] - 155s 110ms/step - loss: 0.0174 - accuracy: 0.9944\n",
      "Epoch 15/100\n",
      "1413/1413 [==============================] - 155s 110ms/step - loss: 0.0176 - accuracy: 0.9944\n",
      "Epoch 16/100\n",
      "1413/1413 [==============================] - 154s 109ms/step - loss: 0.0174 - accuracy: 0.9944\n",
      "Epoch 17/100\n",
      "1413/1413 [==============================] - 155s 109ms/step - loss: 0.0172 - accuracy: 0.9944\n",
      "Epoch 18/100\n",
      "1413/1413 [==============================] - 155s 110ms/step - loss: 0.0175 - accuracy: 0.9943\n",
      "Epoch 19/100\n",
      "1413/1413 [==============================] - 154s 109ms/step - loss: 0.0172 - accuracy: 0.9943\n",
      "Epoch 20/100\n",
      "1413/1413 [==============================] - 159s 112ms/step - loss: 0.0165 - accuracy: 0.9948\n",
      "Epoch 21/100\n",
      "1413/1413 [==============================] - 175s 124ms/step - loss: 0.0178 - accuracy: 0.9943\n",
      "Epoch 22/100\n",
      "1413/1413 [==============================] - 194s 137ms/step - loss: 0.0171 - accuracy: 0.9944\n",
      "Epoch 23/100\n",
      "1413/1413 [==============================] - 189s 134ms/step - loss: 0.0177 - accuracy: 0.9945\n",
      "Epoch 24/100\n",
      "1413/1413 [==============================] - 207s 146ms/step - loss: 0.0171 - accuracy: 0.9945\n",
      "Epoch 25/100\n",
      "1413/1413 [==============================] - 210s 148ms/step - loss: 0.0175 - accuracy: 0.9943\n",
      "Epoch 26/100\n",
      "1413/1413 [==============================] - 198s 140ms/step - loss: 0.0176 - accuracy: 0.9942\n",
      "Epoch 27/100\n",
      "1413/1413 [==============================] - 175s 123ms/step - loss: 0.0172 - accuracy: 0.9945\n",
      "Epoch 28/100\n",
      "1413/1413 [==============================] - 178s 125ms/step - loss: 0.0176 - accuracy: 0.9943\n",
      "Epoch 29/100\n",
      "1413/1413 [==============================] - 182s 129ms/step - loss: 0.0160 - accuracy: 0.9947\n",
      "Epoch 30/100\n",
      "1413/1413 [==============================] - 181s 128ms/step - loss: 0.0162 - accuracy: 0.9948\n",
      "Epoch 31/100\n",
      "1413/1413 [==============================] - 182s 129ms/step - loss: 0.0166 - accuracy: 0.9945\n",
      "Epoch 32/100\n",
      "1413/1413 [==============================] - 180s 127ms/step - loss: 0.0170 - accuracy: 0.9945\n",
      "Epoch 33/100\n",
      "1413/1413 [==============================] - 182s 129ms/step - loss: 0.0169 - accuracy: 0.9945\n",
      "Epoch 34/100\n",
      "1413/1413 [==============================] - 180s 127ms/step - loss: 0.0164 - accuracy: 0.9946\n",
      "Epoch 35/100\n",
      "1413/1413 [==============================] - 174s 123ms/step - loss: 0.0173 - accuracy: 0.9944\n",
      "Epoch 36/100\n",
      "1413/1413 [==============================] - 156s 110ms/step - loss: 0.0163 - accuracy: 0.9947\n",
      "Epoch 37/100\n",
      "1413/1413 [==============================] - 155s 110ms/step - loss: 0.0172 - accuracy: 0.9945\n",
      "Epoch 38/100\n",
      "1413/1413 [==============================] - 156s 111ms/step - loss: 0.0165 - accuracy: 0.9947\n",
      "Epoch 39/100\n",
      "1413/1413 [==============================] - 158s 112ms/step - loss: 0.0171 - accuracy: 0.9943\n",
      "Epoch 40/100\n",
      "1413/1413 [==============================] - 154s 109ms/step - loss: 0.0150 - accuracy: 0.9950\n",
      "Epoch 41/100\n",
      "1413/1413 [==============================] - 156s 110ms/step - loss: 0.0163 - accuracy: 0.9946\n",
      "Epoch 42/100\n",
      "1413/1413 [==============================] - 156s 110ms/step - loss: 0.0164 - accuracy: 0.9946\n",
      "Epoch 43/100\n",
      "1413/1413 [==============================] - 156s 110ms/step - loss: 0.0169 - accuracy: 0.9945\n",
      "Epoch 44/100\n",
      "1413/1413 [==============================] - 157s 111ms/step - loss: 0.0151 - accuracy: 0.9951\n",
      "Epoch 45/100\n",
      "1413/1413 [==============================] - 155s 110ms/step - loss: 0.0168 - accuracy: 0.9945\n",
      "Epoch 46/100\n",
      "1413/1413 [==============================] - 158s 111ms/step - loss: 0.0166 - accuracy: 0.9946\n",
      "Epoch 47/100\n",
      "1413/1413 [==============================] - 156s 110ms/step - loss: 0.0171 - accuracy: 0.9944\n",
      "Epoch 48/100\n",
      "1413/1413 [==============================] - 156s 110ms/step - loss: 0.0166 - accuracy: 0.9945\n",
      "Epoch 49/100\n",
      "1413/1413 [==============================] - 156s 110ms/step - loss: 0.0161 - accuracy: 0.9948\n",
      "Epoch 50/100\n",
      "1413/1413 [==============================] - 158s 112ms/step - loss: 0.0164 - accuracy: 0.9947\n",
      "Epoch 51/100\n",
      "1413/1413 [==============================] - 156s 111ms/step - loss: 0.0171 - accuracy: 0.9943\n",
      "Epoch 52/100\n",
      "1413/1413 [==============================] - 157s 111ms/step - loss: 0.0171 - accuracy: 0.9943\n",
      "Epoch 53/100\n",
      "1413/1413 [==============================] - 160s 113ms/step - loss: 0.0160 - accuracy: 0.9946\n",
      "Epoch 54/100\n",
      "1413/1413 [==============================] - 185s 131ms/step - loss: 0.0160 - accuracy: 0.9948\n",
      "INFO:tensorflow:Assets written to: models/ANN_Tfidf_oEDA-hqfes_100128100/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess tensorflow INFO     Assets written to: models/ANN_Tfidf_oEDA-hqfes_100128100/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9947\n",
      "Testing Accuracy:  0.7350\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = ANN_model(input_dim, neurons=250, hidden=1)\n",
    "model.summary()\n",
    "\n",
    "model_name = \"ANN_Tfidf_oEDA-hqfes_\" + str(epochs) + str(batch) + str(percentage * 100)\n",
    "\n",
    "if import_model and os.path.exists(\"models/\" + model_name):\n",
    "    model = load_model(\"models/\" + model_name)\n",
    "else:\n",
    "    # fit network\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train_eda,\n",
    "        epochs=epochs,\n",
    "        verbose=True,\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "        callbacks=[es],\n",
    "    )\n",
    "    model.save(\"models/\" + model_name)\n",
    "\n",
    "# Print the score\n",
    "train_loss, train_accuracy = model.evaluate(\n",
    "    X_train, y_train_eda, batch_size=128, verbose=False\n",
    ")\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_eda, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_accuracy, name=model_name, index=[\"Accuracy Score\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "breeding-pavilion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T19:19:58.661182Z",
     "start_time": "2021-04-05T19:19:58.610708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultiNB_Count-Raw</th>\n",
       "      <td>0.850505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-Raw</th>\n",
       "      <td>0.855550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-s</th>\n",
       "      <td>0.873340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-es</th>\n",
       "      <td>0.860860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-hqfes</th>\n",
       "      <td>0.728625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf_EDA-hqfes</th>\n",
       "      <td>0.769251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf_oEDA-hqfes</th>\n",
       "      <td>0.768455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN_Tfidf-hqfes_100128100</th>\n",
       "      <td>0.765003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN_Tfidf_oEDA-hqfes_100128100</th>\n",
       "      <td>0.734997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Accuracy Score\n",
       "MultiNB_Count-Raw                     0.850505\n",
       "MultiNB_Tfidf-Raw                     0.855550\n",
       "MultiNB_Tfidf-s                       0.873340\n",
       "MultiNB_Tfidf-es                      0.860860\n",
       "MultiNB_Tfidf-hqfes                   0.728625\n",
       "MultiNB_Tfidf_EDA-hqfes               0.769251\n",
       "MultiNB_Tfidf_oEDA-hqfes              0.768455\n",
       "ANN_Tfidf-hqfes_100128100             0.765003\n",
       "ANN_Tfidf_oEDA-hqfes_100128100        0.734997"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(score_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-angola",
   "metadata": {},
   "source": [
    "Comparing the results of the of the ANN to the MNB model, the accuracy was about the same when trained on the cleaned data without EDA but actually decreased when trained with the EDA data. This is a surprising result and it is unclear why the accuracy might have decreased. Perhaps there are too many augmented entries created by the EDA method and so the network is being over-trained on the training data. The optimum result from the GA was determined on the MNB model and was assumed that it would be the optimum result for the ANN too but maybe that assumption is not correct. Ideally the optimum EDA method could be determined on the ANN but that would be too computationally expensive to test on this PC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-particular",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "The next model to be tested is a Convolutional Neural Network (CNN). A CNN is similar to an ANN in that it is made up of connected layers of nodes but in the CNN the nodes preform convolutions. When using a 2D input such as a matrix, the convolutional layers multiply small sections of the matrix together and sum the results to form a smaller matrix called a feature map. The effect is that the convolutional layers extract complex features from the input data and then can relate these features to a given output. CNNs are used often in machine vision projects but can also be used in NLP projects if the data is processed correctly. \n",
    "\n",
    "The best way to represent text data for a CNN is to use word embedding. Similar to TfidfVectorizer, this involves converting the entries into vectors but this time each word is represented by an n-dimensional vector itself. To convert a word to a vector, an embedding layer is added to the network which encodes the words based on their relationship to one another. A vector for one word should be close to the vector of a related word in n-dimensional space. Embedding can also be done using external algorithms and then fed into the convolution layers of the network directly. Both the keras embedding layer and the Word2Vec algorithm will be tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "inappropriate-eating",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T19:20:00.751643Z",
     "start_time": "2021-04-05T19:19:58.664807Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(entries_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(entries_train)\n",
    "X_test = tokenizer.texts_to_sequences(entries_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "# A max length of 600 was chosen as the majority of entries are no longer than 600 words\n",
    "maxlen = 600\n",
    "\n",
    "X_train = pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding=\"post\", maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "incorrect-logic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T19:20:00.768491Z",
     "start_time": "2021-04-05T19:20:00.756905Z"
    }
   },
   "outputs": [],
   "source": [
    "def CNN_model(\n",
    "    vocab_size, embedding_dim, maxlen, filters=128, kernals=3, hidden=1, neurons=100\n",
    "):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "\n",
    "    for i in range(hidden):\n",
    "        model.add(layers.Conv1D(filters, kernals, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling1D())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(layers.Dense(20, kernel_initializer=\"normal\", activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    # model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-baseline",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "\n",
    "Since each CNN takes about 30 minutes to train even when using Google Colab the hyperparameter space could not be explored extensively. The initial plan was to test the parameters seen below but since training takes so long this was not possible. In the end, the same parameters that the authors used in the EDA paper were chosen for the structure of the CNN so that results generated here could be compared to the results in the paper. The authors of the paper used a single convolutional layer with 128 filters and a kernel size of 5. The convolutional layer is then connected to a dense layer with 100 neurons. The authors use Word2Vec as the input to the convolutional layer but in this case an embedding layer is tested before using Word2Vec to compare the results.     "
   ]
  },
  {
   "cell_type": "raw",
   "id": "sought-classroom",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "model = KerasClassifier(build_fn=CNN_model, epochs=100)\n",
    "\n",
    "# Define the combination of parameters to test\n",
    "filters = [32, 64, 128]\n",
    "kernals = [2, 3, 4]\n",
    "hidden = [1, 2, 3]\n",
    "neurons = [50, 100, 150]\n",
    "param_grid = dict(\n",
    "    vocab_size=[vocab_size],\n",
    "    embedding_dim=[100],\n",
    "    maxlen=[max_len],\n",
    "    filters=filters,\n",
    "    kernals=kernals,\n",
    "    hidden=hidden,\n",
    "    neurons=neurons,\n",
    ")\n",
    "print(param_grid)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    # fit network\n",
    "    grid_result = grid.fit(X_train, y_train, callbacks=[es])\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_[\"mean_test_score\"]\n",
    "stds = grid_result.cv_results_[\"std_test_score\"]\n",
    "params = grid_result.cv_results_[\"params\"]\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-spine",
   "metadata": {},
   "source": [
    "####  Convolutional Neural Network on cleaned data using word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "american-clark",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T20:20:19.124900Z",
     "start_time": "2021-04-05T19:20:00.770267Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 600, 100)          6291500   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 596, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 298, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 38144)             0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               3814500   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 10,172,148\n",
      "Trainable params: 10,172,148\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "118/118 [==============================] - 69s 577ms/step - loss: 2.9210 - accuracy: 0.0812\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 67s 564ms/step - loss: 1.7620 - accuracy: 0.3906\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 67s 566ms/step - loss: 0.9936 - accuracy: 0.6634\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 67s 569ms/step - loss: 0.5332 - accuracy: 0.8387\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 66s 562ms/step - loss: 0.3066 - accuracy: 0.9163\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 68s 575ms/step - loss: 0.1872 - accuracy: 0.9547\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 73s 616ms/step - loss: 0.1177 - accuracy: 0.9736\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 77s 655ms/step - loss: 0.0815 - accuracy: 0.9824\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 69s 588ms/step - loss: 0.0729 - accuracy: 0.9837\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 69s 588ms/step - loss: 0.0628 - accuracy: 0.9871\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 72s 608ms/step - loss: 0.0551 - accuracy: 0.9865\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 66s 563ms/step - loss: 0.0748 - accuracy: 0.9801\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 67s 567ms/step - loss: 0.0805 - accuracy: 0.9786\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 68s 572ms/step - loss: 0.0724 - accuracy: 0.9824\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 67s 570ms/step - loss: 0.0600 - accuracy: 0.9854\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 68s 575ms/step - loss: 0.0506 - accuracy: 0.9867\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 67s 570ms/step - loss: 0.0401 - accuracy: 0.9901\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 67s 570ms/step - loss: 0.0329 - accuracy: 0.9923\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 67s 566ms/step - loss: 0.0283 - accuracy: 0.9936\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 65s 549ms/step - loss: 0.0319 - accuracy: 0.9923\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 59s 498ms/step - loss: 0.0296 - accuracy: 0.9927\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 0.0351 - accuracy: 0.9905\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 60s 508ms/step - loss: 0.0277 - accuracy: 0.9936\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 0.0271 - accuracy: 0.9933\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 0.0220 - accuracy: 0.9944\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 0.0271 - accuracy: 0.9928\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 0.0219 - accuracy: 0.9940\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 60s 511ms/step - loss: 0.0190 - accuracy: 0.9944\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 0.0225 - accuracy: 0.9936\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 0.0205 - accuracy: 0.9944\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 0.0227 - accuracy: 0.9937\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 0.0169 - accuracy: 0.9951\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 0.0179 - accuracy: 0.9951\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 0.0174 - accuracy: 0.9950\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 59s 502ms/step - loss: 0.0152 - accuracy: 0.9953\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 60s 506ms/step - loss: 0.0177 - accuracy: 0.9946\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 59s 500ms/step - loss: 0.0154 - accuracy: 0.9952\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 61s 513ms/step - loss: 0.0147 - accuracy: 0.9957\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 0.0187 - accuracy: 0.9941\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 0.0166 - accuracy: 0.9951\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 62s 525ms/step - loss: 0.0206 - accuracy: 0.9937\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 0.0180 - accuracy: 0.9939\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 60s 504ms/step - loss: 0.0168 - accuracy: 0.9947\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 0.0150 - accuracy: 0.9952\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 61s 520ms/step - loss: 0.0186 - accuracy: 0.9944\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 60s 507ms/step - loss: 0.0179 - accuracy: 0.9946\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 60s 504ms/step - loss: 0.0171 - accuracy: 0.9940\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 60s 504ms/step - loss: 0.0149 - accuracy: 0.9952\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 0.0830 - accuracy: 0.9797\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 0.3624 - accuracy: 0.9173\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 60s 505ms/step - loss: 0.0850 - accuracy: 0.9780\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 59s 504ms/step - loss: 0.0405 - accuracy: 0.9913\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 60s 504ms/step - loss: 0.0234 - accuracy: 0.9940\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 59s 503ms/step - loss: 0.0214 - accuracy: 0.9943\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 60s 509ms/step - loss: 0.0208 - accuracy: 0.9941\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 61s 519ms/step - loss: 0.0186 - accuracy: 0.9940\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 66s 562ms/step - loss: 0.0177 - accuracy: 0.9945\n",
      "INFO:tensorflow:Assets written to: models/CNN_embed-hqfes_100128100/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess tensorflow INFO     Assets written to: models/CNN_embed-hqfes_100128100/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9948\n",
      "Testing Accuracy:  0.6216\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "model = CNN_model(\n",
    "    vocab_size, embedding_dim, maxlen, filters=128, kernals=5, hidden=1, neurons=100\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "model_name = \"CNN_embed-hqfes_\" + str(epochs) + str(batch) + str(percentage * 100)\n",
    "\n",
    "if import_model and os.path.exists(\"models/\" + model_name):\n",
    "    model = load_model(\"models/\" + model_name)\n",
    "else:\n",
    "    # fit network\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        verbose=True,\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "        callbacks=[es],\n",
    "    )\n",
    "    model.save(\"models/\" + model_name)\n",
    "\n",
    "# Print the score\n",
    "train_loss, train_accuracy = model.evaluate(\n",
    "    X_train, y_train, batch_size=128, verbose=False\n",
    ")\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_accuracy, name=model_name, index=[\"Accuracy Score\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-large",
   "metadata": {},
   "source": [
    "After training the CNN, the model produces an accuracy score of ~62% which is lower than even the worst performing MNB model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-disorder",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network on cleaned EDA data using word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dying-bryan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T20:20:38.753071Z",
     "start_time": "2021-04-05T20:20:19.126324Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(entries_train_eda)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(entries_train_eda)\n",
    "X_test = tokenizer.texts_to_sequences(entries_test_eda)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "maxlen = 600\n",
    "\n",
    "X_train = pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding=\"post\", maxlen=maxlen)\n",
    "\n",
    "y_train_eda = np.array([np.array(x) for x in y_train_eda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "latest-flour",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:18:22.761495Z",
     "start_time": "2021-04-05T20:20:38.759538Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 600, 100)          7520500   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 598, 128)          38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 299, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 38272)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               3827300   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 11,388,348\n",
      "Trainable params: 11,388,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1413/1413 [==============================] - 717s 507ms/step - loss: 1.4977 - accuracy: 0.5217\n",
      "Epoch 2/100\n",
      "1413/1413 [==============================] - 647s 458ms/step - loss: 0.1483 - accuracy: 0.9574\n",
      "Epoch 3/100\n",
      "1413/1413 [==============================] - 639s 452ms/step - loss: 0.0902 - accuracy: 0.9745\n",
      "Epoch 4/100\n",
      "1413/1413 [==============================] - 641s 454ms/step - loss: 0.0643 - accuracy: 0.9816\n",
      "Epoch 5/100\n",
      "1413/1413 [==============================] - 665s 471ms/step - loss: 0.0610 - accuracy: 0.9827\n",
      "Epoch 6/100\n",
      "1413/1413 [==============================] - 727s 514ms/step - loss: 0.0466 - accuracy: 0.9862\n",
      "Epoch 7/100\n",
      "1413/1413 [==============================] - 710s 502ms/step - loss: 0.0451 - accuracy: 0.9867\n",
      "Epoch 8/100\n",
      "1413/1413 [==============================] - 697s 493ms/step - loss: 0.0369 - accuracy: 0.9894\n",
      "Epoch 9/100\n",
      "1413/1413 [==============================] - 698s 494ms/step - loss: 0.0387 - accuracy: 0.9890\n",
      "Epoch 10/100\n",
      "1413/1413 [==============================] - 699s 494ms/step - loss: 0.0349 - accuracy: 0.9901\n",
      "Epoch 11/100\n",
      "1413/1413 [==============================] - 698s 494ms/step - loss: 0.0342 - accuracy: 0.9909\n",
      "Epoch 12/100\n",
      "1413/1413 [==============================] - 696s 493ms/step - loss: 0.0307 - accuracy: 0.9914\n",
      "Epoch 13/100\n",
      "1413/1413 [==============================] - 696s 493ms/step - loss: 0.0271 - accuracy: 0.9923\n",
      "Epoch 14/100\n",
      "1413/1413 [==============================] - 699s 495ms/step - loss: 0.0281 - accuracy: 0.9921\n",
      "Epoch 15/100\n",
      "1413/1413 [==============================] - 699s 495ms/step - loss: 0.0292 - accuracy: 0.9920\n",
      "Epoch 16/100\n",
      "1413/1413 [==============================] - 695s 492ms/step - loss: 0.0279 - accuracy: 0.9923\n",
      "Epoch 17/100\n",
      "1413/1413 [==============================] - 693s 490ms/step - loss: 0.0275 - accuracy: 0.9923\n",
      "Epoch 18/100\n",
      "1413/1413 [==============================] - 696s 492ms/step - loss: 0.0218 - accuracy: 0.9935\n",
      "Epoch 19/100\n",
      "1413/1413 [==============================] - 696s 492ms/step - loss: 0.0233 - accuracy: 0.9932\n",
      "Epoch 20/100\n",
      "1413/1413 [==============================] - 698s 494ms/step - loss: 0.0259 - accuracy: 0.9928\n",
      "Epoch 21/100\n",
      "1413/1413 [==============================] - 700s 495ms/step - loss: 0.0220 - accuracy: 0.9937\n",
      "Epoch 22/100\n",
      "1413/1413 [==============================] - 700s 495ms/step - loss: 0.0231 - accuracy: 0.9936\n",
      "Epoch 23/100\n",
      "1413/1413 [==============================] - 700s 495ms/step - loss: 0.0276 - accuracy: 0.9927\n",
      "Epoch 24/100\n",
      "1413/1413 [==============================] - 697s 493ms/step - loss: 0.0246 - accuracy: 0.9932\n",
      "Epoch 25/100\n",
      "1413/1413 [==============================] - 701s 496ms/step - loss: 0.0213 - accuracy: 0.9939\n",
      "Epoch 26/100\n",
      "1413/1413 [==============================] - 698s 494ms/step - loss: 0.0227 - accuracy: 0.9937\n",
      "Epoch 27/100\n",
      "1413/1413 [==============================] - 698s 494ms/step - loss: 0.0234 - accuracy: 0.9936\n",
      "Epoch 28/100\n",
      "1413/1413 [==============================] - 698s 494ms/step - loss: 0.0216 - accuracy: 0.9936\n",
      "Epoch 29/100\n",
      "1413/1413 [==============================] - 695s 492ms/step - loss: 0.0216 - accuracy: 0.9938\n",
      "Epoch 30/100\n",
      "1413/1413 [==============================] - 697s 494ms/step - loss: 0.0192 - accuracy: 0.9944\n",
      "Epoch 31/100\n",
      "1413/1413 [==============================] - 700s 495ms/step - loss: 0.0172 - accuracy: 0.9947\n",
      "Epoch 32/100\n",
      "1413/1413 [==============================] - 697s 493ms/step - loss: 0.0221 - accuracy: 0.9939\n",
      "Epoch 33/100\n",
      "1413/1413 [==============================] - 698s 494ms/step - loss: 0.0257 - accuracy: 0.9935\n",
      "Epoch 34/100\n",
      "1413/1413 [==============================] - 694s 491ms/step - loss: 0.0226 - accuracy: 0.9939\n",
      "Epoch 35/100\n",
      "1413/1413 [==============================] - 694s 491ms/step - loss: 0.0209 - accuracy: 0.9940\n",
      "Epoch 36/100\n",
      "1413/1413 [==============================] - 699s 494ms/step - loss: 0.0210 - accuracy: 0.9939\n",
      "Epoch 37/100\n",
      "1413/1413 [==============================] - 691s 489ms/step - loss: 0.0244 - accuracy: 0.9935\n",
      "Epoch 38/100\n",
      "1413/1413 [==============================] - 696s 492ms/step - loss: 0.0205 - accuracy: 0.9940\n",
      "Epoch 39/100\n",
      "1413/1413 [==============================] - 691s 489ms/step - loss: 0.0188 - accuracy: 0.9943\n",
      "Epoch 40/100\n",
      "1413/1413 [==============================] - 689s 488ms/step - loss: 0.0215 - accuracy: 0.9939\n",
      "Epoch 41/100\n",
      "1413/1413 [==============================] - 695s 492ms/step - loss: 0.0218 - accuracy: 0.9939\n",
      "INFO:tensorflow:Assets written to: models/CNN_embed_oEDA-hqfes_100128100/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess tensorflow INFO     Assets written to: models/CNN_embed_oEDA-hqfes_100128100/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9941\n",
      "Testing Accuracy:  0.6819\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "model = CNN_model(\n",
    "    vocab_size, embedding_dim, maxlen, filters=128, kernals=3, hidden=1, neurons=100\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "model_name = \"CNN_embed_oEDA-hqfes_\" + str(epochs) + str(batch) + str(percentage * 100)\n",
    "\n",
    "if import_model and os.path.exists(\"models/\" + model_name):\n",
    "    model = load_model(\"models/\" + model_name)\n",
    "else:\n",
    "    # fit network\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train_eda,\n",
    "        epochs=epochs,\n",
    "        verbose=True,\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "        callbacks=[es],\n",
    "    )\n",
    "    model.save(\"models/\" + model_name)\n",
    "\n",
    "# Print the score\n",
    "train_loss, train_accuracy = model.evaluate(\n",
    "    X_train, y_train_eda, batch_size=128, verbose=False\n",
    ")\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_eda, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_accuracy, name=model_name, index=[\"Accuracy Score\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-arabic",
   "metadata": {},
   "source": [
    "After applying EDA the accuracy score has increased this time from 62% to 68% which is about the same level of improvement seen when using the MNB model but is still quite a poor result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-capitol",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network on cleaned EDA data using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "returning-simulation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:19:59.744659Z",
     "start_time": "2021-04-06T04:18:22.783117Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess gensim.models.word2vec INFO     collecting all words and their counts\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #10000, processed 1015417 words, keeping 48484 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #20000, processed 2051802 words, keeping 61300 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #30000, processed 3041242 words, keeping 66755 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #40000, processed 4073497 words, keeping 69664 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #50000, processed 5046978 words, keeping 71130 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #60000, processed 6020643 words, keeping 72014 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #70000, processed 7031189 words, keeping 72682 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #80000, processed 8005124 words, keeping 73176 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #90000, processed 9015442 words, keeping 73548 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #100000, processed 10031559 words, keeping 73836 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #110000, processed 11020669 words, keeping 74041 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #120000, processed 12026601 words, keeping 74246 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #130000, processed 13025697 words, keeping 74437 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #140000, processed 14019956 words, keeping 74588 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #150000, processed 15034089 words, keeping 74774 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #160000, processed 16009582 words, keeping 74923 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #170000, processed 17073701 words, keeping 75066 word types\n",
      "MainProcess gensim.models.word2vec INFO     PROGRESS: at sentence #180000, processed 18082130 words, keeping 75192 word types\n",
      "MainProcess gensim.models.word2vec INFO     collected 82789 word types from a corpus of 18550843 raw words and 184510 sentences\n",
      "MainProcess gensim.models.word2vec INFO     Loading a fresh vocabulary\n",
      "MainProcess gensim.models.word2vec INFO     effective_min_count=1 retains 82789 unique words (100% of original 82789, drops 0)\n",
      "MainProcess gensim.models.word2vec INFO     effective_min_count=1 leaves 18550843 word corpus (100% of original 18550843, drops 0)\n",
      "MainProcess gensim.models.word2vec INFO     deleting the raw counts dictionary of 82789 items\n",
      "MainProcess gensim.models.word2vec INFO     sample=0.001 downsamples 9 most-common words\n",
      "MainProcess gensim.models.word2vec INFO     downsampling leaves estimated 18422490 word corpus (99.3% of prior 18550843)\n",
      "MainProcess gensim.models.base_any2vec INFO     estimated required memory for 82789 words and 100 dimensions: 107625700 bytes\n",
      "MainProcess gensim.models.word2vec INFO     resetting layer weights\n",
      "MainProcess gensim.models.base_any2vec INFO     training model with 8 workers on 82789 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 5.41% examples, 1000230 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 11.68% examples, 1075810 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 20.18% examples, 1240538 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 26.56% examples, 1211991 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 33.55% examples, 1222186 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 42.02% examples, 1274786 words/s, in_qsize 13, out_qsize 2\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 50.44% examples, 1312995 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 59.01% examples, 1342000 words/s, in_qsize 16, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 67.35% examples, 1360489 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 75.60% examples, 1377764 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 84.08% examples, 1391263 words/s, in_qsize 16, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 1 - PROGRESS: at 92.15% examples, 1403451 words/s, in_qsize 16, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 7 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 6 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 5 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 4 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 3 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 2 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 1 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 0 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH - 1 : training on 18550843 raw words (18415711 effective words) took 13.0s, 1414196 effective words/s\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 8.68% examples, 1618371 words/s, in_qsize 13, out_qsize 2\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 17.22% examples, 1590998 words/s, in_qsize 13, out_qsize 2\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 25.96% examples, 1581994 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 34.23% examples, 1558431 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 42.84% examples, 1557272 words/s, in_qsize 16, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 50.97% examples, 1549060 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 59.55% examples, 1547018 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 68.19% examples, 1551228 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 76.66% examples, 1547924 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 85.23% examples, 1546553 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 2 - PROGRESS: at 93.22% examples, 1546185 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 7 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 6 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 5 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 4 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 3 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 2 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 0 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH - 2 : training on 18550843 raw words (18416319 effective words) took 11.9s, 1545553 effective words/s\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 8.11% examples, 1509929 words/s, in_qsize 13, out_qsize 2\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 16.53% examples, 1510227 words/s, in_qsize 16, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 24.65% examples, 1506355 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 33.11% examples, 1511092 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 41.50% examples, 1514285 words/s, in_qsize 16, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 49.97% examples, 1514630 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 58.52% examples, 1516852 words/s, in_qsize 16, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 66.66% examples, 1512122 words/s, in_qsize 16, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 74.89% examples, 1510883 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 83.25% examples, 1511678 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 91.49% examples, 1513076 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 3 - PROGRESS: at 99.64% examples, 1514896 words/s, in_qsize 7, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 7 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 6 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 5 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 4 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 3 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 2 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 1 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 0 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH - 3 : training on 18550843 raw words (18415821 effective words) took 12.1s, 1516013 effective words/s\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 7.97% examples, 1476066 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 16.44% examples, 1512147 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 24.68% examples, 1511077 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 32.99% examples, 1506912 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 41.25% examples, 1503512 words/s, in_qsize 12, out_qsize 3\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 49.80% examples, 1511284 words/s, in_qsize 16, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 58.66% examples, 1522527 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 67.48% examples, 1531552 words/s, in_qsize 13, out_qsize 2\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 75.66% examples, 1530260 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 84.25% examples, 1531738 words/s, in_qsize 16, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 4 - PROGRESS: at 92.15% examples, 1530029 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 7 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 6 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 5 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 4 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 3 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 2 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 1 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 0 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH - 4 : training on 18550843 raw words (18416169 effective words) took 12.0s, 1531125 effective words/s\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 8.08% examples, 1489828 words/s, in_qsize 13, out_qsize 2\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 16.50% examples, 1510191 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 24.89% examples, 1522732 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 33.30% examples, 1514999 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 41.68% examples, 1514366 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 50.17% examples, 1518644 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 58.52% examples, 1515733 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 66.76% examples, 1511488 words/s, in_qsize 13, out_qsize 2\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 75.11% examples, 1513907 words/s, in_qsize 16, out_qsize 2\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 83.25% examples, 1509447 words/s, in_qsize 14, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 91.82% examples, 1517335 words/s, in_qsize 15, out_qsize 0\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 7 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 6 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 5 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 4 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 3 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 2 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 1 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH 5 - PROGRESS: at 100.00% examples, 1519455 words/s, in_qsize 0, out_qsize 1\n",
      "MainProcess gensim.models.base_any2vec INFO     worker thread finished; awaiting finish of 0 more threads\n",
      "MainProcess gensim.models.base_any2vec INFO     EPOCH - 5 : training on 18550843 raw words (18415698 effective words) took 12.1s, 1519308 effective words/s\n",
      "MainProcess gensim.models.base_any2vec INFO     training on a 92754215 raw words (92079718 effective words) took 61.3s, 1502955 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 82789\n"
     ]
    }
   ],
   "source": [
    "entries_eda = list(entries_train_eda) + list(entries_test_eda)\n",
    "entries_eda_list = [text.split(\" \") for text in entries_eda]\n",
    "\n",
    "w2v_model = Word2Vec(entries_eda_list, size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(w2v_model.wv.vocab)\n",
    "print(\"Vocabulary size: %d\" % len(words))\n",
    "# print(w2v_model.wv['space'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "trained-sacrifice",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:20:18.235122Z",
     "start_time": "2021-04-06T04:19:59.746976Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(entries_train_eda)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(entries_train_eda)\n",
    "X_test = tokenizer.texts_to_sequences(entries_test_eda)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "# A max length of 600 was chosen as the majority of entries are no longer than 600 words\n",
    "maxlen = 600\n",
    "\n",
    "X_train = pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding=\"post\", maxlen=maxlen)\n",
    "\n",
    "y_train_eda = np.array([np.array(x) for x in y_train_eda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cloudy-upper",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T04:20:18.435033Z",
     "start_time": "2021-04-06T04:20:18.242038Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding[word]\n",
    "    return weight_matrix\n",
    "\n",
    "\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(w2v_model.wv, tokenizer.word_index)\n",
    "\n",
    "# create the embedding layer\n",
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size, 100, weights=[embedding_vectors], input_length=maxlen, trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "silent-lesson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T17:20:30.103750Z",
     "start_time": "2021-04-06T04:20:18.436760Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 600, 100)          7520500   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 596, 128)          64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 7,599,548\n",
      "Trainable params: 79,048\n",
      "Non-trainable params: 7,520,500\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "1413/1413 [==============================] - 509s 360ms/step - loss: 1.2831 - accuracy: 0.6061\n",
      "Epoch 2/100\n",
      "1413/1413 [==============================] - 508s 360ms/step - loss: 0.6242 - accuracy: 0.7993\n",
      "Epoch 3/100\n",
      "1413/1413 [==============================] - 510s 361ms/step - loss: 0.4550 - accuracy: 0.8529\n",
      "Epoch 4/100\n",
      "1413/1413 [==============================] - 510s 361ms/step - loss: 0.3653 - accuracy: 0.8822\n",
      "Epoch 5/100\n",
      "1413/1413 [==============================] - 505s 358ms/step - loss: 0.3161 - accuracy: 0.8966\n",
      "Epoch 6/100\n",
      "1413/1413 [==============================] - 509s 360ms/step - loss: 0.2768 - accuracy: 0.9096\n",
      "Epoch 7/100\n",
      "1413/1413 [==============================] - 508s 359ms/step - loss: 0.2420 - accuracy: 0.9204\n",
      "Epoch 8/100\n",
      "1413/1413 [==============================] - 512s 362ms/step - loss: 0.2200 - accuracy: 0.9287\n",
      "Epoch 9/100\n",
      "1413/1413 [==============================] - 508s 359ms/step - loss: 0.1999 - accuracy: 0.9342\n",
      "Epoch 10/100\n",
      "1413/1413 [==============================] - 511s 361ms/step - loss: 0.1861 - accuracy: 0.9384\n",
      "Epoch 11/100\n",
      "1413/1413 [==============================] - 504s 357ms/step - loss: 0.1794 - accuracy: 0.9405\n",
      "Epoch 12/100\n",
      "1413/1413 [==============================] - 510s 361ms/step - loss: 0.1609 - accuracy: 0.9459\n",
      "Epoch 13/100\n",
      "1413/1413 [==============================] - 509s 360ms/step - loss: 0.1483 - accuracy: 0.9514\n",
      "Epoch 14/100\n",
      "1413/1413 [==============================] - 508s 359ms/step - loss: 0.1438 - accuracy: 0.9523\n",
      "Epoch 15/100\n",
      "1413/1413 [==============================] - 509s 360ms/step - loss: 0.1332 - accuracy: 0.9552\n",
      "Epoch 16/100\n",
      "1413/1413 [==============================] - 511s 361ms/step - loss: 0.1294 - accuracy: 0.9566\n",
      "Epoch 17/100\n",
      "1413/1413 [==============================] - 508s 360ms/step - loss: 0.1238 - accuracy: 0.9585\n",
      "Epoch 18/100\n",
      "1413/1413 [==============================] - 510s 361ms/step - loss: 0.1207 - accuracy: 0.9592\n",
      "Epoch 19/100\n",
      "1413/1413 [==============================] - 508s 359ms/step - loss: 0.1169 - accuracy: 0.9604\n",
      "Epoch 20/100\n",
      "1413/1413 [==============================] - 508s 359ms/step - loss: 0.1128 - accuracy: 0.9618\n",
      "Epoch 21/100\n",
      "1413/1413 [==============================] - 507s 359ms/step - loss: 0.1129 - accuracy: 0.9617\n",
      "Epoch 22/100\n",
      "1413/1413 [==============================] - 507s 359ms/step - loss: 0.1086 - accuracy: 0.9630\n",
      "Epoch 23/100\n",
      "1413/1413 [==============================] - 508s 360ms/step - loss: 0.1000 - accuracy: 0.9660\n",
      "Epoch 24/100\n",
      "1413/1413 [==============================] - 509s 360ms/step - loss: 0.0973 - accuracy: 0.9669\n",
      "Epoch 25/100\n",
      "1413/1413 [==============================] - 511s 362ms/step - loss: 0.1046 - accuracy: 0.9645\n",
      "Epoch 26/100\n",
      "1413/1413 [==============================] - 507s 359ms/step - loss: 0.1099 - accuracy: 0.9644\n",
      "Epoch 27/100\n",
      "1413/1413 [==============================] - 508s 360ms/step - loss: 0.0899 - accuracy: 0.9692\n",
      "Epoch 28/100\n",
      "1413/1413 [==============================] - 509s 360ms/step - loss: 0.0966 - accuracy: 0.9675\n",
      "Epoch 29/100\n",
      "1413/1413 [==============================] - 510s 361ms/step - loss: 0.0885 - accuracy: 0.9703\n",
      "Epoch 30/100\n",
      "1413/1413 [==============================] - 541s 383ms/step - loss: 0.0943 - accuracy: 0.9689\n",
      "Epoch 31/100\n",
      "1413/1413 [==============================] - 554s 392ms/step - loss: 0.0878 - accuracy: 0.9705\n",
      "Epoch 32/100\n",
      "1413/1413 [==============================] - 539s 382ms/step - loss: 0.0892 - accuracy: 0.9701\n",
      "Epoch 33/100\n",
      "1413/1413 [==============================] - 534s 378ms/step - loss: 0.0804 - accuracy: 0.9728\n",
      "Epoch 34/100\n",
      "1413/1413 [==============================] - 548s 388ms/step - loss: 0.0862 - accuracy: 0.9713\n",
      "Epoch 35/100\n",
      "1413/1413 [==============================] - 544s 385ms/step - loss: 0.0911 - accuracy: 0.9703\n",
      "Epoch 36/100\n",
      "1413/1413 [==============================] - 527s 373ms/step - loss: 0.0785 - accuracy: 0.9741\n",
      "Epoch 37/100\n",
      "1413/1413 [==============================] - 548s 388ms/step - loss: 0.0798 - accuracy: 0.9729\n",
      "Epoch 38/100\n",
      "1413/1413 [==============================] - 557s 394ms/step - loss: 0.0859 - accuracy: 0.9717\n",
      "Epoch 39/100\n",
      "1413/1413 [==============================] - 556s 393ms/step - loss: 0.0750 - accuracy: 0.9751\n",
      "Epoch 40/100\n",
      "1413/1413 [==============================] - 532s 376ms/step - loss: 0.0785 - accuracy: 0.9739\n",
      "Epoch 41/100\n",
      "1413/1413 [==============================] - 526s 372ms/step - loss: 0.0786 - accuracy: 0.9743\n",
      "Epoch 42/100\n",
      "1413/1413 [==============================] - 562s 398ms/step - loss: 0.0780 - accuracy: 0.9745\n",
      "Epoch 43/100\n",
      "1413/1413 [==============================] - 580s 410ms/step - loss: 0.0669 - accuracy: 0.9777\n",
      "Epoch 44/100\n",
      "1413/1413 [==============================] - 542s 383ms/step - loss: 0.0765 - accuracy: 0.9749\n",
      "Epoch 45/100\n",
      "1413/1413 [==============================] - 549s 388ms/step - loss: 0.0756 - accuracy: 0.9751\n",
      "Epoch 46/100\n",
      "1413/1413 [==============================] - 541s 383ms/step - loss: 0.0725 - accuracy: 0.9763\n",
      "Epoch 47/100\n",
      "1413/1413 [==============================] - 549s 389ms/step - loss: 0.0729 - accuracy: 0.9761\n",
      "Epoch 48/100\n",
      "1413/1413 [==============================] - 574s 406ms/step - loss: 0.0798 - accuracy: 0.9743\n",
      "Epoch 49/100\n",
      "1413/1413 [==============================] - 567s 402ms/step - loss: 0.0709 - accuracy: 0.9772\n",
      "Epoch 50/100\n",
      "1413/1413 [==============================] - 576s 407ms/step - loss: 0.0706 - accuracy: 0.9772\n",
      "Epoch 51/100\n",
      "1413/1413 [==============================] - 556s 393ms/step - loss: 0.0971 - accuracy: 0.9728\n",
      "Epoch 52/100\n",
      "1413/1413 [==============================] - 561s 397ms/step - loss: 0.0713 - accuracy: 0.9769\n",
      "Epoch 53/100\n",
      "1413/1413 [==============================] - 582s 412ms/step - loss: 0.0740 - accuracy: 0.9765\n",
      "Epoch 54/100\n",
      "1413/1413 [==============================] - 575s 407ms/step - loss: 0.0667 - accuracy: 0.9786\n",
      "Epoch 55/100\n",
      "1413/1413 [==============================] - 542s 384ms/step - loss: 0.0686 - accuracy: 0.9776\n",
      "Epoch 56/100\n",
      "1413/1413 [==============================] - 537s 380ms/step - loss: 0.0805 - accuracy: 0.9751\n",
      "Epoch 57/100\n",
      "1413/1413 [==============================] - 541s 383ms/step - loss: 0.0672 - accuracy: 0.9782\n",
      "Epoch 58/100\n",
      "1413/1413 [==============================] - 552s 390ms/step - loss: 0.0768 - accuracy: 0.9764\n",
      "Epoch 59/100\n",
      "1413/1413 [==============================] - 550s 390ms/step - loss: 0.0613 - accuracy: 0.9804\n",
      "Epoch 60/100\n",
      "1413/1413 [==============================] - 549s 389ms/step - loss: 0.0652 - accuracy: 0.9791\n",
      "Epoch 61/100\n",
      "1413/1413 [==============================] - 547s 387ms/step - loss: 0.0648 - accuracy: 0.9794\n",
      "Epoch 62/100\n",
      "1413/1413 [==============================] - 546s 386ms/step - loss: 0.0618 - accuracy: 0.9798\n",
      "Epoch 63/100\n",
      "1413/1413 [==============================] - 559s 396ms/step - loss: 0.0640 - accuracy: 0.9798\n",
      "Epoch 64/100\n",
      "1413/1413 [==============================] - 559s 396ms/step - loss: 0.0672 - accuracy: 0.9788\n",
      "Epoch 65/100\n",
      "1413/1413 [==============================] - 541s 383ms/step - loss: 0.0598 - accuracy: 0.9811\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1413/1413 [==============================] - 543s 384ms/step - loss: 0.0602 - accuracy: 0.9809\n",
      "Epoch 67/100\n",
      "1413/1413 [==============================] - 542s 384ms/step - loss: 0.0658 - accuracy: 0.9798\n",
      "Epoch 68/100\n",
      "1413/1413 [==============================] - 541s 383ms/step - loss: 0.0630 - accuracy: 0.9806\n",
      "Epoch 69/100\n",
      "1413/1413 [==============================] - 552s 391ms/step - loss: 0.0610 - accuracy: 0.9811\n",
      "Epoch 70/100\n",
      "1413/1413 [==============================] - 603s 427ms/step - loss: 0.0634 - accuracy: 0.9802\n",
      "Epoch 71/100\n",
      "1413/1413 [==============================] - 563s 399ms/step - loss: 0.0573 - accuracy: 0.9820\n",
      "Epoch 72/100\n",
      "1413/1413 [==============================] - 551s 390ms/step - loss: 0.0609 - accuracy: 0.9809\n",
      "Epoch 73/100\n",
      "1413/1413 [==============================] - 548s 388ms/step - loss: 0.0598 - accuracy: 0.9815\n",
      "Epoch 74/100\n",
      "1413/1413 [==============================] - 504s 357ms/step - loss: 0.0838 - accuracy: 0.9761\n",
      "Epoch 75/100\n",
      "1413/1413 [==============================] - 480s 340ms/step - loss: 0.0606 - accuracy: 0.9813\n",
      "Epoch 76/100\n",
      "1413/1413 [==============================] - 481s 341ms/step - loss: 0.0647 - accuracy: 0.9800\n",
      "Epoch 77/100\n",
      "1413/1413 [==============================] - 483s 342ms/step - loss: 0.0760 - accuracy: 0.9780\n",
      "Epoch 78/100\n",
      "1413/1413 [==============================] - 486s 344ms/step - loss: 0.0591 - accuracy: 0.9818\n",
      "Epoch 79/100\n",
      "1413/1413 [==============================] - 495s 350ms/step - loss: 0.0574 - accuracy: 0.9820\n",
      "Epoch 80/100\n",
      "1413/1413 [==============================] - 497s 352ms/step - loss: 0.0628 - accuracy: 0.9812\n",
      "Epoch 81/100\n",
      "1413/1413 [==============================] - 541s 383ms/step - loss: 0.0592 - accuracy: 0.9822\n",
      "Epoch 82/100\n",
      "1413/1413 [==============================] - 566s 400ms/step - loss: 0.0588 - accuracy: 0.9816\n",
      "Epoch 83/100\n",
      "1413/1413 [==============================] - 513s 363ms/step - loss: 0.0604 - accuracy: 0.9816\n",
      "Epoch 84/100\n",
      "1413/1413 [==============================] - 488s 346ms/step - loss: 0.0598 - accuracy: 0.9821\n",
      "Epoch 85/100\n",
      "1413/1413 [==============================] - 494s 349ms/step - loss: 0.0585 - accuracy: 0.9824\n",
      "Epoch 86/100\n",
      "1413/1413 [==============================] - 555s 393ms/step - loss: 0.0622 - accuracy: 0.9814\n",
      "Epoch 87/100\n",
      "1413/1413 [==============================] - 509s 360ms/step - loss: 0.0618 - accuracy: 0.9818\n",
      "Epoch 88/100\n",
      "1413/1413 [==============================] - 480s 340ms/step - loss: 0.0613 - accuracy: 0.9816\n",
      "INFO:tensorflow:Assets written to: models/CNN_word2vec_oEDA-hqfes_100128100/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainProcess tensorflow INFO     Assets written to: models/CNN_word2vec_oEDA-hqfes_100128100/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9845\n",
      "Testing Accuracy:  0.6710\n"
     ]
    }
   ],
   "source": [
    "# Custom model which uses Word2Vec\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(layers.Conv1D(128, 5, activation=\"relu\"))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation=\"relu\"))\n",
    "model.add(layers.Dense(20, kernel_initializer=\"normal\", activation=\"softmax\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "model_name = (\n",
    "    \"CNN_word2vec_oEDA-hqfes_\" + str(epochs) + str(batch) + str(percentage * 100)\n",
    ")\n",
    "\n",
    "if import_model and os.path.exists(\"models/\" + model_name):\n",
    "    model = load_model(\"models/\" + model_name)\n",
    "else:\n",
    "    # fit network\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train_eda,\n",
    "        epochs=epochs,\n",
    "        verbose=True,\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "        callbacks=[es],\n",
    "    )\n",
    "    model.save(\"models/\" + model_name)\n",
    "\n",
    "# Print the score\n",
    "train_loss, train_accuracy = model.evaluate(\n",
    "    X_train, y_train_eda, batch_size=128, verbose=False\n",
    ")\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_eda, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_accuracy, name=model_name, index=[\"Accuracy Score\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "seasonal-study",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T17:20:30.121195Z",
     "start_time": "2021-04-06T17:20:30.108000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultiNB_Count-Raw</th>\n",
       "      <td>0.850505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-Raw</th>\n",
       "      <td>0.855550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-s</th>\n",
       "      <td>0.873340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-es</th>\n",
       "      <td>0.860860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-hqfes</th>\n",
       "      <td>0.728625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf_EDA-hqfes</th>\n",
       "      <td>0.769251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf_oEDA-hqfes</th>\n",
       "      <td>0.768455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN_Tfidf-hqfes_100128100</th>\n",
       "      <td>0.765003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN_Tfidf_oEDA-hqfes_100128100</th>\n",
       "      <td>0.734997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN_embed-hqfes_100128100</th>\n",
       "      <td>0.621614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN_embed_oEDA-hqfes_100128100</th>\n",
       "      <td>0.681891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN_word2vec_oEDA-hqfes_100128100</th>\n",
       "      <td>0.671004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Accuracy Score\n",
       "MultiNB_Count-Raw                        0.850505\n",
       "MultiNB_Tfidf-Raw                        0.855550\n",
       "MultiNB_Tfidf-s                          0.873340\n",
       "MultiNB_Tfidf-es                         0.860860\n",
       "MultiNB_Tfidf-hqfes                      0.728625\n",
       "MultiNB_Tfidf_EDA-hqfes                  0.769251\n",
       "MultiNB_Tfidf_oEDA-hqfes                 0.768455\n",
       "ANN_Tfidf-hqfes_100128100                0.765003\n",
       "ANN_Tfidf_oEDA-hqfes_100128100           0.734997\n",
       "CNN_embed-hqfes_100128100                0.621614\n",
       "CNN_embed_oEDA-hqfes_100128100           0.681891\n",
       "CNN_word2vec_oEDA-hqfes_100128100        0.671004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(score_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-wyoming",
   "metadata": {},
   "source": [
    "Comparing the accuracy of the CNN to the other models tested it appears to have a worse performance but in this case the EDA method has increased the performance of the model. The poor performance is a surprising result as CNNs are frequently used in text classification problems and have been shown to give good results. Perhaps the parameters chosen are not a good fit for this dataset and in the future the parameter space could be explored to better tune the model.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-botswana",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network\n",
    "\n",
    "Initially the plan was to test and compare the performance of a Recurrent Neural Network (RNN) against the other models. Unfortunately while training, the RNN would consistently crash, regardless of what structure was picked for the model. In the future, the RNN could be tested and trained on a more powerful PC with better GPU support. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "sweet-mainstream",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T13:00:51.021678Z",
     "start_time": "2021-03-31T13:00:49.529355Z"
    }
   },
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(entries_train_eda)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(entries_train_eda)\n",
    "X_test = tokenizer.texts_to_sequences(entries_test_eda)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "# A max length of 600 was chosen as the majority of entries are no longer than 600 words\n",
    "maxlen = 600\n",
    "\n",
    "X_train = pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding=\"post\", maxlen=maxlen)\n",
    "\n",
    "y_train_eda = np.array([np.array(x) for x in y_train_eda])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "current-steam",
   "metadata": {},
   "source": [
    "def RNN_model(vocab_size, embedding_dim, maxlen, lstm_units=32, hidden=1, neurons=100):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "\n",
    "    for i in range(hidden):\n",
    "        model.add(layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(neurons, activation=\"relu\"))\n",
    "    model.add(layers.Dense(20, kernel_initializer=\"normal\", activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    # model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-yellow",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network on cleaned EDA data using word embedding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "knowing-ballot",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:09:18.072703Z",
     "start_time": "2021-03-31T13:00:51.022861Z"
    }
   },
   "source": [
    "embedding_dim = 100\n",
    "model = RNN_model(\n",
    "    vocab_size, embedding_dim, maxlen, lstm_units=32, hidden=1, neurons=100\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "model_name = \"RNN_embed_oEDA-hqfes_\" + str(epochs) + str(batch) + str(percentage * 100)\n",
    "\n",
    "if import_model and os.path.exists(\"models/\" + model_name):\n",
    "    model = load_model(\"models/\" + model_name)\n",
    "else:\n",
    "    # fit network\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train_eda,\n",
    "        epochs=epochs,\n",
    "        verbose=True,\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "        callbacks=[es],\n",
    "    )\n",
    "    model.save(\"models/\" + model_name)\n",
    "\n",
    "# Print the score\n",
    "train_loss, train_accuracy = model.evaluate(\n",
    "    X_train, y_train_eda, batch_size=128, verbose=False\n",
    ")\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_eda, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_accuracy, name=model_name, index=[\"Accuracy Score\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-rolling",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network on cleaned EDA data using Word2Vec"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sunset-sailing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:09:28.595963Z",
     "start_time": "2021-03-31T14:09:18.087429Z"
    }
   },
   "source": [
    "entries_eda = list(entries_train_eda) + list(entries_test_eda)\n",
    "entries_eda_list = [text.split(\" \") for text in entries_eda]\n",
    "\n",
    "w2v_model = Word2Vec(entries_eda_list, size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(w2v_model.wv.vocab)\n",
    "print(\"Vocabulary size: %d\" % len(words))\n",
    "# print(w2v_model.wv['space'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adjusted-michigan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:09:30.198577Z",
     "start_time": "2021-03-31T14:09:28.597394Z"
    }
   },
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(entries_train_eda)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(entries_train_eda)\n",
    "X_test = tokenizer.texts_to_sequences(entries_test_eda)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "# A max length of 600 was chosen as the majority of entries are no longer than 600 words\n",
    "maxlen = 600\n",
    "\n",
    "X_train = pad_sequences(X_train, padding=\"post\", maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding=\"post\", maxlen=maxlen)\n",
    "\n",
    "y_train_eda = np.array([np.array(x) for x in y_train_eda])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "suitable-swaziland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:09:30.270163Z",
     "start_time": "2021-03-31T14:09:30.203195Z"
    }
   },
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding[word]\n",
    "    return weight_matrix\n",
    "\n",
    "\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(w2v_model.wv, tokenizer.word_index)\n",
    "\n",
    "# create the embedding layer\n",
    "embedding_layer = layers.Embedding(\n",
    "    vocab_size, 100, weights=[embedding_vectors], input_length=maxlen, trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "swedish-command",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-31T12:36:49.181Z"
    }
   },
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(layers.Bidirectional(LSTM(32, return_sequences=False)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "model.add(layers.Dense(20, kernel_initializer=\"normal\", activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "model_name = (\n",
    "    \"RNN_word2vec_oEDA-hqfes_\" + str(epochs) + str(batch) + str(percentage * 100)\n",
    ")\n",
    "\n",
    "if import_model and os.path.exists(\"models/\" + model_name):\n",
    "    model = load_model(\"models/\" + model_name)\n",
    "else:\n",
    "    # fit network\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train_eda,\n",
    "        epochs=epochs,\n",
    "        verbose=True,\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "        callbacks=[es],\n",
    "    )\n",
    "    model.save(\"models/\" + model_name)\n",
    "\n",
    "# Print the score\n",
    "train_loss, train_accuracy = model.evaluate(\n",
    "    X_train, y_train_eda, batch_size=128, verbose=False\n",
    ")\n",
    "print(\"Training Accuracy: {:.4f}\".format(train_accuracy))\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_eda, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(test_accuracy))\n",
    "\n",
    "# Append the score to the leaderboard\n",
    "score_table = score_table.append(\n",
    "    pd.Series(test_accuracy, name=model_name, index=[\"Accuracy Score\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-option",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "banned-suicide",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T17:20:30.152233Z",
     "start_time": "2021-04-06T17:20:30.122801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultiNB_Count-Raw</th>\n",
       "      <td>0.850505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-Raw</th>\n",
       "      <td>0.855550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-s</th>\n",
       "      <td>0.873340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-es</th>\n",
       "      <td>0.860860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf-hqfes</th>\n",
       "      <td>0.728625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf_EDA-hqfes</th>\n",
       "      <td>0.769251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultiNB_Tfidf_oEDA-hqfes</th>\n",
       "      <td>0.768455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN_Tfidf-hqfes_100128100</th>\n",
       "      <td>0.765003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANN_Tfidf_oEDA-hqfes_100128100</th>\n",
       "      <td>0.734997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN_embed-hqfes_100128100</th>\n",
       "      <td>0.621614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN_embed_oEDA-hqfes_100128100</th>\n",
       "      <td>0.681891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNN_word2vec_oEDA-hqfes_100128100</th>\n",
       "      <td>0.671004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Accuracy Score\n",
       "MultiNB_Count-Raw                        0.850505\n",
       "MultiNB_Tfidf-Raw                        0.855550\n",
       "MultiNB_Tfidf-s                          0.873340\n",
       "MultiNB_Tfidf-es                         0.860860\n",
       "MultiNB_Tfidf-hqfes                      0.728625\n",
       "MultiNB_Tfidf_EDA-hqfes                  0.769251\n",
       "MultiNB_Tfidf_oEDA-hqfes                 0.768455\n",
       "ANN_Tfidf-hqfes_100128100                0.765003\n",
       "ANN_Tfidf_oEDA-hqfes_100128100           0.734997\n",
       "CNN_embed-hqfes_100128100                0.621614\n",
       "CNN_embed_oEDA-hqfes_100128100           0.681891\n",
       "CNN_word2vec_oEDA-hqfes_100128100        0.671004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(score_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-semester",
   "metadata": {},
   "source": [
    "The final scores have produced a surprising result. The ANN trained on the data without EDA performed the best with a accuracy score of 77.5% but only marginally beating the Multinomial Naive Bayes model using EDA with 76.8%. The CNN performed the worst of the three models only achieving a score of 68% on the embedded data without EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-sessions",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "While the ANN produced adequate results, the deep learning models have not preformed as well as expected. There are many reasons for this, the first one is that no extensive hyperparameter tuning has been done on these models. Different model structures were tested for the ANN but only one CNN structure was tested. With some tuning perhaps the accuracy scores could be improved but since each round of training takes anywhere from 30 minutes to 4 hours an extensive search is not possible using this computer or using Google Colab. The optimizer and the neuron activation function parameters used by the models were picked based on previous work done but perhaps by trying different types, some improvements could also be made. \n",
    "\n",
    "As well as the models hyperparameters, the values picked for the EDA method itself have also not been greatly tuned. The genetic algorithm began to converge on a result after a coarse search but perhaps if the process was run for longer a different set of parameters might have emerged. Efforts could be made to parallelize the EDA function to speed up the search but that is outside the scope of this project. Also the GA was tested on the MNB model with the assumption that the optimum result for that model would be the same for the others. This assumption might not be correct as the ANN showed a decrease in performance after applying EDA.   \n",
    "\n",
    "Finally, perhaps this classification problem might not be suited for the use of deep learning methods. In the future, the performance of more simple machine learning models could be tested such as Support Vector Machine (SVM) and Random Forrest Classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-jacob",
   "metadata": {},
   "source": [
    "## References "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-appreciation",
   "metadata": {},
   "source": [
    "- https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes\n",
    "- https://www.upgrad.com/blog/multinomial-naive-bayes-explained/\n",
    "- https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "- https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "- https://scikit-learn.org/stable/user_guide.html\n",
    "- https://scikit-learn.org/stable/modules/classes.html\n",
    "- https://machinelearningmastery.com/automate-machine-learning-workflows-pipelines-python-scikit-learn/\n",
    "- https://arxiv.org/pdf/1901.11196.pdf\n",
    "- https://github.com/jasonwei20/eda_nlp\n",
    "- https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "- https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "- https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "- https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
    "- https://machinelearningmastery.com/best-practices-document-classification-deep-learning/\n",
    "- https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/\n",
    "- https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/\n",
    "- https://www.analyticsvidhya.com/blog/2020/11/text-cleaning-nltk-library/\n",
    "- https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "- https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n",
    "- https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/\n",
    "- https://realpython.com/python-keras-text-classification/#a-primer-on-deep-neural-networks\n",
    "- https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "- https://towardsdatascience.com/text-classification-in-python-dd95d264c802\n",
    "- https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "- https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "- https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "- https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/datasets/_twenty_newsgroups.py#L329\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html?highlight=news#sklearn.datasets.fetch_20newsgroups\n",
    "- https://stackoverflow.com/questions/61961042/indices201-0-8-is-out-of-order-many-sparse-ops-require-sorted-indices-use\n",
    "- https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/\n",
    "- https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
    "- https://ai.stackexchange.com/questions/5546/what-is-the-difference-between-a-convolutional-neural-network-and-a-regular-neur#:~:text=A%20convolutional%20neural%20network%20is,is%20closer%20to%20the%20truth).\n",
    "- https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D\n",
    "- https://www.tensorflow.org/versions\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "- https://machinelearningmastery.com/recommendations-for-deep-learning-neural-network-practitioners/\n",
    "- https://towardsdatascience.com/a-walkthrough-of-convolutional-neural-network-7f474f91d7bd\n",
    "- https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "- https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "- https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "- https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
    "- https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "- https://stats.stackexchange.com/questions/31060/bag-of-words-vs-vector-space-model\n",
    "- https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "- https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "- https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "- https://towardsdatascience.com/keras-callbacks-and-how-to-save-your-model-from-overtraining-244fc1de8608"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
